{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1Elio6XF-zSHRQ7AKVjBpTkMCoVXHIrgp","authorship_tag":"ABX9TyPjEMJfHWP2oA+gO5uj2lqM"},"gpuClass":"standard","accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":7705447,"sourceType":"datasetVersion","datasetId":4498585}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gabrielfcarvalho/yolov8-sorghum-detection?scriptVersionId=164408393\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **GPU**","metadata":{"id":"kqZFu7lcAdRq"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"Czu63cCsAftH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682715944055,"user_tz":180,"elapsed":1074,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"644275e4-3652-4081-998c-c404c47c627d","execution":{"iopub.status.busy":"2024-02-26T15:39:22.973054Z","iopub.execute_input":"2024-02-26T15:39:22.973749Z","iopub.status.idle":"2024-02-26T15:39:23.967419Z","shell.execute_reply.started":"2024-02-26T15:39:22.973719Z","shell.execute_reply":"2024-02-26T15:39:23.966217Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Mon Feb 26 15:39:23 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0              26W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Installing Ultralytics**","metadata":{"id":"Ky3lxBDcAjLj"}},{"cell_type":"code","source":"! pip install ultralytics","metadata":{"id":"NLKlVnA-An2Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685044683387,"user_tz":180,"elapsed":5659,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"761596ff-95fb-4c92-c8ad-b62ded3fcd73","execution":{"iopub.status.busy":"2024-02-26T15:39:43.971814Z","iopub.execute_input":"2024-02-26T15:39:43.972521Z","iopub.status.idle":"2024-02-26T15:39:58.212297Z","shell.execute_reply.started":"2024-02-26T15:39:43.972485Z","shell.execute_reply":"2024-02-26T15:39:58.21139Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.1.18-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.4)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nCollecting thop>=0.1.1 (from ultralytics)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2023.12.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.1.18-py3-none-any.whl (716 kB)\n\u001b[2K   \u001b[90m\u001b[0m \u001b[32m716.0/716.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.1.18\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Modifying yaml file**","metadata":{}},{"cell_type":"code","source":"%cp /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/data.yaml /kaggle/working/data.yaml","metadata":{"execution":{"iopub.status.busy":"2024-02-26T15:52:06.160903Z","iopub.execute_input":"2024-02-26T15:52:06.161736Z","iopub.status.idle":"2024-02-26T15:52:07.120677Z","shell.execute_reply.started":"2024-02-26T15:52:06.161702Z","shell.execute_reply":"2024-02-26T15:52:07.119445Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Let's create a new data.yaml content with the updated paths as requested by the user.\n\nnew_data_yaml_content = \"\"\"train: /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/train/images\nval: /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/valid/images\ntest: /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images\n\nnc: 1\nnames: ['c']\n\"\"\"\n\n# Save the content to a new file\nyaml_file_path = '/kaggle/working/data.yaml'\nwith open(yaml_file_path, 'w') as file:\n    file.write(new_data_yaml_content)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T15:52:28.995372Z","iopub.execute_input":"2024-02-26T15:52:28.99577Z","iopub.status.idle":"2024-02-26T15:52:29.00229Z","shell.execute_reply.started":"2024-02-26T15:52:28.995737Z","shell.execute_reply":"2024-02-26T15:52:29.001386Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **Training Model**","metadata":{"id":"QEtDb-LBHD2I"}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\n\nos.environ['WANDB_DISABLED'] = 'true'\n\n# Load a model\nmodel = YOLO('yolov8s.pt')  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data='/kaggle/working/data.yaml', epochs=100, imgsz=416, cache=True, device=0, patience=0, batch=16)","metadata":{"id":"V8leWnJZHFxF","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1682709915619,"user_tz":180,"elapsed":482660,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"e7066e45-a631-427e-fab9-6f997efe24b1","execution":{"iopub.status.busy":"2024-02-26T15:59:41.443445Z","iopub.execute_input":"2024-02-26T15:59:41.444385Z","iopub.status.idle":"2024-02-26T16:09:09.151376Z","shell.execute_reply.started":"2024-02-26T15:59:41.444329Z","shell.execute_reply":"2024-02-26T16:09:09.149586Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.18  Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/kaggle/working/data.yaml, epochs=100, time=None, patience=0, batch=16, imgsz=416, save=True, save_period=-1, cache=True, device=0, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \nModel summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n\nTransferred 349/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路路\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240226_160101-oipqij17</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gabrielfc2102/YOLOv8/runs/oipqij17' target=\"_blank\">train4</a></strong> to <a href='https://wandb.ai/gabrielfc2102/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gabrielfc2102/YOLOv8' target=\"_blank\">https://wandb.ai/gabrielfc2102/YOLOv8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gabrielfc2102/YOLOv8/runs/oipqij17' target=\"_blank\">https://wandb.ai/gabrielfc2102/YOLOv8/runs/oipqij17</a>"},"metadata":{}},{"name":"stdout","text":"Freezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/train/labels... 723 images, 0 backgrounds, 0 corrupt: 100%|| 723/723 [00:02<00:00, 327.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING 锔 Cache directory /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/train is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB True): 100%|| 723/723 [00:01<00:00, 381.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/valid/labels... 25 images, 0 backgrounds, 0 corrupt: 100%|| 25/25 [00:00<00:00, 231.92it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING 锔 Cache directory /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB True): 100%|| 25/25 [00:00<00:00, 187.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/train4/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \nImage sizes 416 train, 416 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train4\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      1/100      1.98G      2.064      1.807      1.701         39        416: 100%|| 46/46 [00:12<00:00,  3.56it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:01<00:00,  1.31s/it]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.483      0.823      0.549      0.276\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      2/100      2.09G      1.545      1.096      1.304         39        416: 100%|| 46/46 [00:07<00:00,  5.84it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.323      0.637      0.299      0.135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      3/100      2.07G       1.52      1.035      1.313         47        416: 100%|| 46/46 [00:07<00:00,  5.91it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.62      0.575      0.586      0.284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      4/100      2.06G      1.485      1.012      1.288         60        416: 100%|| 46/46 [00:07<00:00,  5.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.42it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.69      0.613      0.688      0.341\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      5/100      2.05G      1.482      1.015      1.288         45        416: 100%|| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.18it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.779      0.684      0.814       0.43\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      6/100      2.06G      1.453     0.9796      1.267         58        416: 100%|| 46/46 [00:07<00:00,  5.90it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.779      0.745      0.844       0.47\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      7/100      2.05G       1.44     0.9839      1.268         43        416: 100%|| 46/46 [00:07<00:00,  6.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.81      0.806      0.874      0.463\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      8/100      2.06G      1.426      0.944      1.266         26        416: 100%|| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.746      0.804      0.849      0.476\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      9/100      2.06G       1.41     0.9258      1.251         37        416: 100%|| 46/46 [00:07<00:00,  5.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.719      0.764      0.783      0.421\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     10/100      2.06G      1.407     0.9257      1.236         34        416: 100%|| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.829      0.779      0.892      0.508\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     11/100      2.06G      1.384     0.9018      1.231         41        416: 100%|| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.774      0.774      0.861      0.464\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     12/100      2.06G      1.367     0.8906       1.22         43        416: 100%|| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.763       0.84      0.839      0.429\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     13/100      2.05G      1.381     0.9217       1.24         36        416: 100%|| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.82      0.703      0.846      0.478\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     14/100      2.06G      1.356      0.899      1.216         36        416: 100%|| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.786      0.781      0.833      0.467\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     15/100      1.97G      1.361     0.9016      1.225         24        416: 100%|| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.771      0.788      0.838      0.481\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     16/100      2.04G      1.334     0.8717      1.196         29        416: 100%|| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.683      0.868      0.818      0.452\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     17/100      1.97G      1.338     0.9025      1.208         32        416: 100%|| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.838      0.825      0.882      0.508\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     18/100      2.06G       1.34     0.8637      1.209         27        416: 100%|| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.86      0.811      0.891      0.488\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     19/100      2.06G      1.314     0.8522      1.197         30        416: 100%|| 46/46 [00:07<00:00,  6.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.761      0.751      0.804      0.438\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     20/100      2.06G      1.341     0.8726      1.213         45        416: 100%|| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.814      0.797      0.868      0.474\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     21/100      2.04G      1.316     0.8503        1.2         38        416: 100%|| 46/46 [00:07<00:00,  5.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.776      0.802      0.862      0.476\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     22/100      2.06G       1.32     0.8265      1.206         36        416: 100%|| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.826      0.854      0.897      0.483\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     23/100      2.05G      1.323     0.8497      1.199         51        416: 100%|| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.829       0.83       0.88      0.517\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     24/100      2.06G      1.328     0.8226      1.198         41        416: 100%|| 46/46 [00:07<00:00,  6.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.791      0.868      0.888      0.504\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     25/100      1.97G      1.302     0.8158      1.191         42        416: 100%|| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.84       0.79      0.883      0.518\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     26/100      2.05G      1.312     0.8227      1.192         39        416: 100%|| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.804      0.811      0.875       0.49\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     27/100      1.97G      1.304     0.8459      1.192         33        416: 100%|| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.864       0.83      0.894      0.516\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     28/100      2.05G      1.288     0.8058      1.184         47        416: 100%|| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.827      0.811        0.9      0.503\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     29/100      1.98G      1.277     0.7917      1.168         41        416: 100%|| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.803      0.811      0.853      0.475\n","output_type":"stream"},{"name":"stderr","text":"\n     32/100      2.06G      1.301     0.8025      1.181         33        416: 100%|| 46/46 [00:07<00:00,  6.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.845      0.792      0.882      0.522\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     33/100      1.97G      1.275     0.7758      1.166         54        416: 100%|| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.832      0.844       0.87       0.53\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     34/100      2.05G      1.261     0.7716      1.167         42        416: 100%|| 46/46 [00:07<00:00,  6.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.787      0.855       0.88      0.513\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     35/100      2.06G      1.263     0.7687      1.158         38        416: 100%|| 46/46 [00:07<00:00,  6.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.863      0.821      0.892        0.5\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     36/100      1.98G      1.248      0.771      1.152         38        416: 100%|| 46/46 [00:07<00:00,  6.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.824      0.825      0.869      0.499\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     37/100      2.06G      1.266     0.7632       1.16         50        416: 100%|| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.871      0.821      0.908      0.513\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     38/100      2.06G      1.263     0.7657      1.164         24        416: 100%|| 46/46 [00:07<00:00,  6.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.863      0.821      0.885       0.48\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     39/100      2.06G      1.246     0.7602      1.156         29        416: 100%|| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.857      0.868      0.894      0.526\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     40/100      1.98G      1.229     0.7366      1.141         38        416: 100%|| 46/46 [00:07<00:00,  6.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.86      0.816      0.887      0.503\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     41/100      2.05G      1.239     0.7506      1.151         33        416: 100%|| 46/46 [00:07<00:00,  5.94it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.765      0.854       0.87      0.481\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     42/100      2.05G      1.223     0.7336      1.136         59        416: 100%|| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  5.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.876      0.867      0.911      0.531\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     43/100      1.97G      1.228     0.7234       1.15         52        416: 100%|| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.851      0.838      0.896      0.515\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     44/100      2.06G      1.228     0.7308      1.155         38        416: 100%|| 46/46 [00:07<00:00,  6.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  4.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.84      0.802      0.864      0.478\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     45/100      1.97G      1.213     0.7129      1.136         49        416: 100%|| 46/46 [00:07<00:00,  6.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.809      0.816      0.852      0.478\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     46/100      1.97G      1.214     0.7323       1.14         52        416: 100%|| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 1/1 [00:00<00:00,  6.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.835      0.769       0.87      0.496\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     47/100      1.98G      1.204     0.7301      1.136        223        416:  63%|   | 29/46 [00:04<00:02,  5.87it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/data.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:644\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:376\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    375\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:261\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    260\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/utils/loss.py:214\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n\u001b[1;32m    213\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 214\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m gt_labels, gt_bboxes \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# cls, xyxy\u001b[39;00m\n\u001b[1;32m    216\u001b[0m mask_gt \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mgt_(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/utils/loss.py:184\u001b[0m, in \u001b[0;36mv8DetectionLoss.preprocess\u001b[0;34m(self, targets, batch_size, scale_tensor)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n:\n\u001b[1;32m    183\u001b[0m             out[j, :n] \u001b[38;5;241m=\u001b[39m targets[matches, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 184\u001b[0m     out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mxywh2xyxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"# **Model Validation**","metadata":{"id":"7044GQnbgmoN"}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('/kaggle/working/runs/detect/train4/weights/best.pt')  # load a custom model\nmetrics = model.val(split = 'test')  # no arguments needed, dataset and settings remembered","metadata":{"id":"KsnaEQ9TkLEo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682709247038,"user_tz":180,"elapsed":49216,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"fa4eb179-f72c-4e0e-9b8c-1a7e38e4411f","execution":{"iopub.status.busy":"2024-02-26T16:09:34.61255Z","iopub.execute_input":"2024-02-26T16:09:34.613255Z","iopub.status.idle":"2024-02-26T16:09:42.402252Z","shell.execute_reply.started":"2024-02-26T16:09:34.6132Z","shell.execute_reply":"2024-02-26T16:09:42.400949Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.18  Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/labels... 32 images, 0 backgrounds, 0 corrupt: 100%|| 32/32 [00:00<00:00, 335.11it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING 锔 Cache directory /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 2/2 [00:02<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all         32        257      0.792      0.846      0.864      0.509\nSpeed: 0.1ms preprocess, 7.4ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#metrics.box.map    # map50-95\nmetrics.box.map50  # map50\n#metrics.box.map75  # map75\n#metrics.box.maps   # a list contains map50-95 of each category","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGBhc4O7W8yY","executionInfo":{"status":"ok","timestamp":1682708984415,"user_tz":180,"elapsed":3,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"943ca8e7-3e88-4fe4-c249-764999ae4c69"},"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":["0.8554853986895027"]},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# **Testing the model in the images**","metadata":{"id":"J17quNpbrUUo"}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('/kaggle/working/runs/detect/train4/weights/best.pt')  # load a custom model\nmodel.predict('/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images', save=True, imgsz=416, conf=0.5, show_labels=True, show_conf=True)","metadata":{"id":"cYqJg12UrY1A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685044715042,"user_tz":180,"elapsed":1955,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"34d6a666-6e16-4abb-85b6-5e418653a8b2","execution":{"iopub.status.busy":"2024-02-26T16:10:39.96764Z","iopub.execute_input":"2024-02-26T16:10:39.968423Z","iopub.status.idle":"2024-02-26T16:10:41.086447Z","shell.execute_reply.started":"2024-02-26T16:10:39.968356Z","shell.execute_reply":"2024-02-26T16:10:41.085329Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nimage 1/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_17_png.rf.2cbfe9d703e1046a6bd252360814d983.jpg: 416x416 9 cs, 6.7ms\nimage 2/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_20_png.rf.b42eb18de55a877b0e327be4c2596bf9.jpg: 416x416 9 cs, 8.9ms\nimage 3/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_25_png.rf.c782ccc3452d9c73651d90080c7fa248.jpg: 416x416 9 cs, 6.7ms\nimage 4/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_28_png.rf.9749fcf39aea7932df9a36f8645b7381.jpg: 416x416 12 cs, 6.4ms\nimage 5/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_33_png.rf.8206f2b80887a56d425abdfcaf22ab49.jpg: 416x416 12 cs, 6.5ms\nimage 6/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_35_png.rf.d5391a0424a348be3bef0010be20801f.jpg: 416x416 9 cs, 6.4ms\nimage 7/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_48_png.rf.fc2f106cbecb2aca31605850989bfd78.jpg: 416x416 9 cs, 6.4ms\nimage 8/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_105_png.rf.654dfc59c220ab8e879d185ae947fd0e.jpg: 416x416 8 cs, 6.4ms\nimage 9/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_121_png.rf.f57d5de666f070bc561ff20c721c8e8f.jpg: 416x416 9 cs, 6.3ms\nimage 10/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_122_png.rf.e0a857770d48399eb4e8ed75f0469b9e.jpg: 416x416 9 cs, 6.4ms\nimage 11/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_124_png.rf.880cf55a745bd87522d9ac1cf09334be.jpg: 416x416 7 cs, 6.5ms\nimage 12/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_12_png.rf.896d9610e8b3109bb4d498202c89c2de.jpg: 416x416 5 cs, 6.3ms\nimage 13/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_134_png.rf.41ef2a70982eb3781162c1f9428e7a34.jpg: 416x416 7 cs, 6.6ms\nimage 14/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_160_png.rf.3e1a825bd9b8af6aedb164f527c1b412.jpg: 416x416 18 cs, 6.8ms\nimage 15/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_166_png.rf.9cdd602f0662e345ab143cb4ebc58edb.jpg: 416x416 11 cs, 6.7ms\nimage 16/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_188_png.rf.9745ecfcfe31aa4fb165e008c2898deb.jpg: 416x416 9 cs, 6.7ms\nimage 17/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_18_png.rf.945f918b7846052452e031aec2028822.jpg: 416x416 14 cs, 9.3ms\nimage 18/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_192_png.rf.aa94ed3b709c89860f1a52d0c02b975f.jpg: 416x416 16 cs, 6.5ms\nimage 19/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_196_png.rf.83dca24251b5280cc1084e7fb722e8c9.jpg: 416x416 9 cs, 6.7ms\nimage 20/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_44_png.rf.a23efb05fa7977f94870fa3886fec577.jpg: 416x416 8 cs, 6.5ms\nimage 21/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_53_png.rf.9a18f731eaaf83ab30927ca37379e142.jpg: 416x416 8 cs, 6.5ms\nimage 22/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_75_png.rf.23155c3496205e4094459310f857b293.jpg: 416x416 10 cs, 6.3ms\nimage 23/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_78_png.rf.3ae8d098be942fa2848c56989bdeda92.jpg: 416x416 12 cs, 6.6ms\nimage 24/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_82_png.rf.73e51476d5cdf1f15e0006114582c3a2.jpg: 416x416 5 cs, 6.4ms\nimage 25/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_89_png.rf.21b9d634f712eb1263ae6891e20da1a1.jpg: 416x416 5 cs, 6.7ms\nimage 26/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_95_png.rf.143171f3e7e6d6f9540fef48d833d8af.jpg: 416x416 8 cs, 6.6ms\nimage 27/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_09_png.rf.c47d7daf1c117acb79a82f0b16edbfae.jpg: 416x416 12 cs, 6.5ms\nimage 28/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_11_png.rf.c764badc553ebd7c9893e0be843e134f.jpg: 416x416 9 cs, 6.5ms\nimage 29/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_16_png.rf.a64a46de9434e21c7455d71d209369dd.jpg: 416x416 11 cs, 6.5ms\nimage 30/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_18_png.rf.b65f0023274748b9a404e347de55d35d.jpg: 416x416 10 cs, 6.4ms\nimage 31/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_25_png.rf.5ea0dd56e58ae897e3fbb1565808c72c.jpg: 416x416 6 cs, 6.5ms\nimage 32/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_48_png.rf.a9d3e2d8f0ae863c1589fcd8f21c9c3c.jpg: 416x416 6 cs, 6.3ms\nSpeed: 1.1ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\nResults saved to \u001b[1mruns/detect/predict\u001b[0m\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 72, 122, 122],\n         [ 54, 102, 104],\n         [ 43,  81,  86],\n         ...,\n         [ 57,  65,  82],\n         [ 57,  65,  82],\n         [ 53,  61,  78]],\n \n        [[ 55,  99, 100],\n         [ 54,  95,  98],\n         [ 41,  75,  81],\n         ...,\n         [ 58,  66,  83],\n         [ 50,  58,  75],\n         [ 42,  50,  67]],\n \n        [[ 46,  79,  82],\n         [ 50,  80,  85],\n         [ 30,  56,  63],\n         ...,\n         [ 48,  55,  74],\n         [ 36,  43,  62],\n         [ 26,  33,  52]],\n \n        ...,\n \n        [[ 10,  70,  62],\n         [ 11,  72,  62],\n         [ 20,  82,  70],\n         ...,\n         [ 48,  57,  77],\n         [ 50,  57,  77],\n         [ 44,  51,  71]],\n \n        [[ 45, 119, 101],\n         [ 41, 116,  95],\n         [ 35, 110,  88],\n         ...,\n         [ 33,  42,  62],\n         [ 47,  54,  74],\n         [ 48,  55,  75]],\n \n        [[ 72, 154, 131],\n         [ 62, 145, 120],\n         [ 42, 126,  98],\n         ...,\n         [ 34,  43,  63],\n         [ 41,  48,  68],\n         [ 38,  45,  65]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_17_png.rf.2cbfe9d703e1046a6bd252360814d983.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3616085052490234, 'inference': 6.691932678222656, 'postprocess': 1.91497802734375},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 44,  58,  56],\n         [ 44,  58,  56],\n         [ 68,  77,  80],\n         ...,\n         [ 82,  89, 109],\n         [ 73,  79,  98],\n         [ 68,  74,  93]],\n \n        [[ 39,  53,  52],\n         [ 31,  45,  44],\n         [ 45,  57,  59],\n         ...,\n         [ 85,  92, 112],\n         [ 76,  82, 101],\n         [ 77,  83, 102]],\n \n        [[ 23,  38,  41],\n         [ 21,  36,  39],\n         [ 35,  49,  55],\n         ...,\n         [ 90,  97, 117],\n         [ 81,  87, 106],\n         [ 89,  95, 114]],\n \n        ...,\n \n        [[ 75, 132, 124],\n         [ 93, 152, 144],\n         [129, 190, 180],\n         ...,\n         [ 89,  96, 121],\n         [ 66,  73, 100],\n         [ 59,  66,  93]],\n \n        [[ 60, 112, 105],\n         [ 59, 113, 106],\n         [ 69, 126, 117],\n         ...,\n         [ 99, 108, 135],\n         [ 81,  89, 118],\n         [ 80,  88, 117]],\n \n        [[ 29,  80,  73],\n         [ 36,  89,  80],\n         [ 34,  89,  80],\n         ...,\n         [107, 116, 143],\n         [ 89,  97, 126],\n         [ 75,  83, 112]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_20_png.rf.b42eb18de55a877b0e327be4c2596bf9.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.466512680053711, 'inference': 8.887052536010742, 'postprocess': 1.3806819915771484},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 39,  42,  46],\n         [ 41,  44,  48],\n         [ 43,  46,  50],\n         ...,\n         [ 94, 142, 118],\n         [104, 150, 128],\n         [110, 153, 132]],\n \n        [[ 40,  43,  48],\n         [ 40,  43,  47],\n         [ 36,  39,  44],\n         ...,\n         [ 79, 121, 103],\n         [ 83, 122, 106],\n         [ 79, 116, 100]],\n \n        [[ 30,  33,  41],\n         [ 32,  36,  41],\n         [ 29,  32,  40],\n         ...,\n         [ 58,  89,  82],\n         [ 61,  89,  83],\n         [ 56,  84,  78]],\n \n        ...,\n \n        [[ 83,  83,  99],\n         [ 66,  67,  81],\n         [ 49,  50,  64],\n         ...,\n         [ 64,  72,  89],\n         [ 63,  70,  87],\n         [ 60,  64,  82]],\n \n        [[104, 104, 122],\n         [ 89,  89, 105],\n         [ 74,  74,  90],\n         ...,\n         [ 74,  91, 104],\n         [ 69,  85,  98],\n         [ 64,  80,  93]],\n \n        [[ 89,  89, 107],\n         [ 83,  83, 101],\n         [ 79,  79,  95],\n         ...,\n         [ 62,  84,  96],\n         [ 73,  92, 105],\n         [ 88, 107, 120]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_25_png.rf.c782ccc3452d9c73651d90080c7fa248.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.8106231689453125, 'inference': 6.710529327392578, 'postprocess': 1.2676715850830078},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 98, 129, 130],\n         [ 99, 129, 130],\n         [126, 146, 151],\n         ...,\n         [ 69, 158, 132],\n         [ 55, 149, 124],\n         [ 55, 151, 127]],\n \n        [[ 84, 118, 118],\n         [ 87, 118, 119],\n         [ 99, 124, 128],\n         ...,\n         [ 38, 122,  94],\n         [ 27, 114,  88],\n         [ 35, 124,  98]],\n \n        [[ 56,  96,  95],\n         [ 58,  96,  96],\n         [ 57,  91,  91],\n         ...,\n         [  2,  72,  41],\n         [  0,  66,  36],\n         [  9,  83,  53]],\n \n        ...,\n \n        [[ 63,  70,  87],\n         [ 63,  70,  87],\n         [ 66,  73,  90],\n         ...,\n         [ 61,  86,  96],\n         [ 70,  92, 103],\n         [ 75,  97, 108]],\n \n        [[ 73,  80,  97],\n         [ 71,  78,  95],\n         [ 70,  77,  94],\n         ...,\n         [ 65,  89, 101],\n         [ 71,  93, 105],\n         [ 66,  88, 100]],\n \n        [[ 72,  79,  96],\n         [ 73,  80,  97],\n         [ 73,  80,  97],\n         ...,\n         [ 73,  96, 111],\n         [ 77,  98, 113],\n         [ 66,  87, 102]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_28_png.rf.9749fcf39aea7932df9a36f8645b7381.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7531642913818359, 'inference': 6.363153457641602, 'postprocess': 1.2357234954833984},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 72,  89, 102],\n         [ 66,  83,  96],\n         [ 61,  74,  90],\n         ...,\n         [ 21,  38,  47],\n         [ 37,  53,  60],\n         [ 54,  70,  77]],\n \n        [[ 68,  81,  95],\n         [ 65,  78,  92],\n         [ 60,  71,  85],\n         ...,\n         [ 39,  55,  61],\n         [ 41,  57,  63],\n         [ 49,  64,  67]],\n \n        [[ 76,  82,  95],\n         [ 67,  73,  86],\n         [ 51,  57,  70],\n         ...,\n         [ 71,  87,  86],\n         [ 64,  78,  77],\n         [ 57,  71,  69]],\n \n        ...,\n \n        [[119, 154, 188],\n         [113, 150, 184],\n         [109, 146, 180],\n         ...,\n         [ 91, 138, 169],\n         [ 94, 141, 172],\n         [ 87, 134, 165]],\n \n        [[129, 161, 197],\n         [115, 147, 183],\n         [ 96, 132, 168],\n         ...,\n         [ 92, 138, 169],\n         [ 95, 141, 172],\n         [ 90, 136, 167]],\n \n        [[127, 156, 193],\n         [110, 142, 178],\n         [ 93, 127, 163],\n         ...,\n         [ 92, 138, 169],\n         [ 92, 136, 167],\n         [103, 147, 178]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_33_png.rf.8206f2b80887a56d425abdfcaf22ab49.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2936592102050781, 'inference': 6.475210189819336, 'postprocess': 1.2366771697998047},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 45,  41,  52],\n         [ 47,  46,  56],\n         [ 52,  50,  62],\n         ...,\n         [ 75,  84, 104],\n         [ 85,  92, 112],\n         [ 92,  99, 119]],\n \n        [[ 36,  32,  43],\n         [ 41,  40,  50],\n         [ 48,  48,  60],\n         ...,\n         [ 59,  68,  88],\n         [ 71,  78,  98],\n         [ 80,  87, 107]],\n \n        [[ 33,  32,  42],\n         [ 36,  35,  45],\n         [ 39,  39,  51],\n         ...,\n         [ 45,  52,  72],\n         [ 53,  60,  80],\n         [ 61,  68,  88]],\n \n        ...,\n \n        [[ 15,  18,  26],\n         [ 44,  47,  55],\n         [ 83,  85,  93],\n         ...,\n         [ 70,  82, 100],\n         [ 57,  69,  87],\n         [ 50,  62,  80]],\n \n        [[ 17,  21,  26],\n         [ 42,  46,  51],\n         [ 80,  83,  88],\n         ...,\n         [ 67,  79,  97],\n         [ 49,  61,  79],\n         [ 35,  47,  65]],\n \n        [[ 15,  20,  23],\n         [ 32,  37,  40],\n         [ 68,  71,  76],\n         ...,\n         [ 64,  76,  94],\n         [ 45,  57,  75],\n         [ 29,  41,  59]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_35_png.rf.d5391a0424a348be3bef0010be20801f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7011890411376953, 'inference': 6.379604339599609, 'postprocess': 1.2273788452148438},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 61,  66,  87],\n         [ 61,  66,  87],\n         [ 57,  64,  84],\n         ...,\n         [ 71,  86, 112],\n         [ 70,  81, 108],\n         [ 63,  74, 101]],\n \n        [[ 59,  64,  85],\n         [ 61,  66,  87],\n         [ 63,  68,  89],\n         ...,\n         [ 66,  81, 107],\n         [ 71,  82, 109],\n         [ 72,  83, 110]],\n \n        [[ 58,  62,  81],\n         [ 60,  66,  85],\n         [ 67,  73,  92],\n         ...,\n         [ 74,  89, 115],\n         [ 81,  92, 119],\n         [ 85,  96, 123]],\n \n        ...,\n \n        [[ 61,  68,  83],\n         [ 62,  69,  84],\n         [ 60,  67,  82],\n         ...,\n         [101, 117, 140],\n         [113, 127, 149],\n         [120, 134, 156]],\n \n        [[ 58,  65,  80],\n         [ 63,  70,  85],\n         [ 65,  72,  87],\n         ...,\n         [ 93, 109, 132],\n         [108, 122, 144],\n         [119, 133, 155]],\n \n        [[ 54,  61,  76],\n         [ 63,  70,  85],\n         [ 69,  76,  91],\n         ...,\n         [ 84, 100, 123],\n         [ 93, 107, 129],\n         [101, 115, 137]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_48_png.rf.fc2f106cbecb2aca31605850989bfd78.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2359619140625, 'inference': 6.382942199707031, 'postprocess': 1.2233257293701172},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[106, 125, 146],\n         [ 67,  88, 109],\n         [ 35,  57,  82],\n         ...,\n         [ 79,  85,  96],\n         [ 85,  90,  99],\n         [ 78,  83,  92]],\n \n        [[ 91, 109, 132],\n         [ 66,  87, 109],\n         [ 46,  68,  93],\n         ...,\n         [ 71,  80,  90],\n         [ 77,  84,  93],\n         [ 72,  79,  88]],\n \n        [[ 84, 104, 129],\n         [ 77,  99, 124],\n         [ 73,  96, 122],\n         ...,\n         [ 50,  62,  72],\n         [ 59,  70,  78],\n         [ 58,  69,  77]],\n \n        ...,\n \n        [[ 92, 112, 147],\n         [ 99, 120, 152],\n         [119, 140, 172],\n         ...,\n         [ 37,  49,  53],\n         [ 43,  55,  59],\n         [ 49,  62,  64]],\n \n        [[ 96, 118, 154],\n         [105, 127, 162],\n         [130, 150, 185],\n         ...,\n         [ 37,  49,  53],\n         [ 42,  54,  58],\n         [ 43,  58,  61]],\n \n        [[112, 136, 172],\n         [108, 132, 168],\n         [123, 143, 178],\n         ...,\n         [ 42,  54,  60],\n         [ 39,  54,  57],\n         [ 37,  52,  55]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_105_png.rf.654dfc59c220ab8e879d185ae947fd0e.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7236003875732422, 'inference': 6.4373016357421875, 'postprocess': 1.224517822265625},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 30,  65,  75],\n         [ 68, 102, 115],\n         [ 89, 122, 142],\n         ...,\n         [122, 147, 179],\n         [109, 134, 166],\n         [103, 128, 160]],\n \n        [[ 49,  85,  93],\n         [ 68, 103, 116],\n         [ 81, 114, 133],\n         ...,\n         [108, 133, 165],\n         [ 88, 113, 145],\n         [ 84, 109, 141]],\n \n        [[ 58,  96, 101],\n         [ 75, 112, 120],\n         [ 94, 130, 146],\n         ...,\n         [108, 133, 165],\n         [ 87, 112, 144],\n         [ 85, 110, 142]],\n \n        ...,\n \n        [[ 77, 144, 139],\n         [ 81, 147, 142],\n         [ 90, 151, 147],\n         ...,\n         [ 91, 113, 148],\n         [109, 131, 166],\n         [ 92, 114, 149]],\n \n        [[ 62, 134, 122],\n         [ 76, 145, 134],\n         [100, 166, 155],\n         ...,\n         [ 76,  98, 133],\n         [111, 133, 169],\n         [114, 136, 172]],\n \n        [[ 51, 124, 108],\n         [ 71, 142, 126],\n         [106, 173, 158],\n         ...,\n         [ 61,  83, 118],\n         [ 99, 121, 157],\n         [135, 157, 193]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_121_png.rf.f57d5de666f070bc561ff20c721c8e8f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2378692626953125, 'inference': 6.333112716674805, 'postprocess': 1.2252330780029297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 67,  88, 115],\n         [ 75,  98, 124],\n         [ 95, 118, 144],\n         ...,\n         [111, 136, 168],\n         [105, 130, 162],\n         [ 91, 116, 148]],\n \n        [[ 68,  89, 116],\n         [ 78, 101, 127],\n         [ 98, 123, 149],\n         ...,\n         [119, 144, 176],\n         [113, 138, 170],\n         [104, 129, 161]],\n \n        [[ 81, 103, 131],\n         [ 91, 113, 141],\n         [105, 129, 157],\n         ...,\n         [122, 147, 179],\n         [114, 139, 171],\n         [109, 134, 166]],\n \n        ...,\n \n        [[ 80, 102, 137],\n         [ 83, 105, 140],\n         [ 85, 108, 140],\n         ...,\n         [100, 125, 159],\n         [101, 126, 160],\n         [104, 129, 163]],\n \n        [[ 71,  93, 128],\n         [ 80, 102, 137],\n         [ 79, 102, 134],\n         ...,\n         [101, 126, 160],\n         [102, 127, 161],\n         [105, 130, 164]],\n \n        [[ 76,  98, 133],\n         [ 90, 112, 147],\n         [ 87, 110, 142],\n         ...,\n         [ 99, 124, 158],\n         [ 98, 123, 157],\n         [ 99, 124, 158]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_122_png.rf.e0a857770d48399eb4e8ed75f0469b9e.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7317066192626953, 'inference': 6.426334381103516, 'postprocess': 1.2581348419189453},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[109, 132, 147],\n         [ 89, 113, 125],\n         [ 72,  95, 103],\n         ...,\n         [ 84,  89, 110],\n         [ 61,  64,  85],\n         [ 47,  50,  71]],\n \n        [[109, 141, 152],\n         [104, 136, 142],\n         [ 89, 119, 124],\n         ...,\n         [ 64,  71,  91],\n         [ 57,  62,  83],\n         [ 58,  63,  84]],\n \n        [[114, 165, 167],\n         [117, 167, 165],\n         [105, 150, 147],\n         ...,\n         [ 47,  55,  78],\n         [ 44,  50,  73],\n         [ 44,  50,  73]],\n \n        ...,\n \n        [[ 58, 108, 114],\n         [ 61, 112, 115],\n         [ 65, 116, 119],\n         ...,\n         [ 76, 101, 135],\n         [105, 129, 165],\n         [117, 143, 179]],\n \n        [[ 68, 113, 147],\n         [ 73, 118, 151],\n         [ 74, 118, 147],\n         ...,\n         [ 94, 119, 153],\n         [110, 134, 170],\n         [119, 143, 179]],\n \n        [[ 71, 112, 161],\n         [ 81, 123, 170],\n         [ 84, 124, 166],\n         ...,\n         [118, 141, 173],\n         [118, 142, 178],\n         [124, 148, 184]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_124_png.rf.880cf55a745bd87522d9ac1cf09334be.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2211799621582031, 'inference': 6.511211395263672, 'postprocess': 1.2645721435546875},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 91, 106, 138],\n         [ 83,  98, 130],\n         [ 92, 110, 141],\n         ...,\n         [110, 142, 183],\n         [115, 147, 188],\n         [117, 149, 190]],\n \n        [[105, 120, 152],\n         [ 92, 110, 141],\n         [ 86, 104, 135],\n         ...,\n         [ 82, 115, 154],\n         [100, 130, 171],\n         [111, 144, 183]],\n \n        [[117, 134, 167],\n         [102, 121, 154],\n         [ 82, 101, 134],\n         ...,\n         [ 57,  89, 125],\n         [ 85, 114, 153],\n         [106, 138, 174]],\n \n        ...,\n \n        [[ 67,  84, 117],\n         [ 78,  97, 130],\n         [ 90, 109, 142],\n         ...,\n         [ 81, 107, 137],\n         [ 81, 107, 137],\n         [ 83, 109, 139]],\n \n        [[ 72,  89, 122],\n         [ 75,  94, 127],\n         [ 78,  97, 130],\n         ...,\n         [ 70,  97, 124],\n         [ 76, 103, 130],\n         [ 82, 109, 136]],\n \n        [[ 86, 103, 136],\n         [ 74,  93, 126],\n         [ 61,  80, 113],\n         ...,\n         [ 65,  92, 119],\n         [ 76, 103, 130],\n         [ 86, 113, 140]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_12_png.rf.896d9610e8b3109bb4d498202c89c2de.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2507438659667969, 'inference': 6.315946578979492, 'postprocess': 1.2869834899902344},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[109, 155, 143],\n         [110, 156, 144],\n         [105, 152, 143],\n         ...,\n         [ 79, 100, 128],\n         [ 75,  96, 124],\n         [ 66,  87, 115]],\n \n        [[112, 157, 148],\n         [ 91, 136, 127],\n         [ 72, 119, 110],\n         ...,\n         [ 88, 109, 137],\n         [ 79, 100, 128],\n         [ 64,  85, 113]],\n \n        [[125, 169, 162],\n         [ 90, 134, 127],\n         [ 63, 110, 102],\n         ...,\n         [ 95, 116, 144],\n         [ 86, 107, 135],\n         [ 76,  97, 125]],\n \n        ...,\n \n        [[ 90, 125, 135],\n         [ 98, 133, 143],\n         [100, 134, 147],\n         ...,\n         [ 57,  66,  86],\n         [ 50,  56,  79],\n         [ 40,  49,  69]],\n \n        [[121, 157, 165],\n         [121, 157, 165],\n         [117, 149, 160],\n         ...,\n         [ 44,  51,  70],\n         [ 52,  57,  78],\n         [ 65,  72,  91]],\n \n        [[127, 163, 169],\n         [128, 164, 170],\n         [126, 159, 168],\n         ...,\n         [ 36,  42,  61],\n         [ 58,  64,  83],\n         [ 91,  97, 116]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_134_png.rf.41ef2a70982eb3781162c1f9428e7a34.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2407302856445312, 'inference': 6.552219390869141, 'postprocess': 1.2218952178955078},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[111, 135, 159],\n         [ 86, 110, 134],\n         [ 64,  88, 112],\n         ...,\n         [ 99, 141, 116],\n         [ 87, 126, 100],\n         [ 72, 109,  83]],\n \n        [[ 68,  92, 116],\n         [ 65,  89, 113],\n         [ 65,  89, 113],\n         ...,\n         [ 86, 131, 105],\n         [ 69, 110,  83],\n         [ 48,  87,  61]],\n \n        [[ 35,  59,  83],\n         [ 42,  66,  90],\n         [ 54,  78, 102],\n         ...,\n         [ 62, 111,  83],\n         [ 33,  81,  53],\n         [  9,  54,  27]],\n \n        ...,\n \n        [[143, 184, 179],\n         [136, 177, 172],\n         [130, 171, 166],\n         ...,\n         [ 59,  82,  84],\n         [ 62,  89,  93],\n         [ 74, 103, 107]],\n \n        [[ 95, 136, 138],\n         [ 86, 127, 129],\n         [ 78, 119, 121],\n         ...,\n         [ 59,  75,  82],\n         [ 76,  95, 103],\n         [ 93, 114, 122]],\n \n        [[ 59, 102, 105],\n         [ 48,  91,  94],\n         [ 42,  82,  87],\n         ...,\n         [ 67,  79,  89],\n         [ 69,  85,  97],\n         [ 70,  88,  99]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_160_png.rf.3e1a825bd9b8af6aedb164f527c1b412.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7176399230957031, 'inference': 6.788492202758789, 'postprocess': 1.3086795806884766},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[104, 128, 158],\n         [118, 142, 172],\n         [118, 142, 172],\n         ...,\n         [ 96, 115, 148],\n         [ 95, 114, 147],\n         [ 98, 117, 150]],\n \n        [[120, 144, 174],\n         [119, 143, 173],\n         [106, 130, 160],\n         ...,\n         [ 90, 108, 139],\n         [ 89, 107, 138],\n         [ 91, 109, 140]],\n \n        [[120, 144, 174],\n         [108, 132, 162],\n         [ 91, 115, 145],\n         ...,\n         [ 81,  97, 126],\n         [ 87, 103, 132],\n         [ 89, 105, 134]],\n \n        ...,\n \n        [[113, 133, 164],\n         [107, 127, 158],\n         [101, 119, 150],\n         ...,\n         [ 93, 107, 129],\n         [ 91, 105, 127],\n         [ 81,  98, 119]],\n \n        [[113, 132, 165],\n         [105, 124, 157],\n         [ 96, 113, 146],\n         ...,\n         [ 92, 105, 127],\n         [ 91, 104, 126],\n         [ 81,  95, 117]],\n \n        [[102, 121, 154],\n         [ 92, 111, 144],\n         [ 84, 101, 134],\n         ...,\n         [ 81,  92, 114],\n         [ 78,  91, 113],\n         [ 71,  84, 106]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_166_png.rf.9cdd602f0662e345ab143cb4ebc58edb.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7441043853759766, 'inference': 6.674051284790039, 'postprocess': 1.2545585632324219},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[131, 153, 181],\n         [126, 148, 176],\n         [125, 149, 179],\n         ...,\n         [ 86, 113, 140],\n         [ 89, 115, 145],\n         [ 89, 115, 145]],\n \n        [[130, 152, 180],\n         [124, 146, 174],\n         [122, 146, 176],\n         ...,\n         [ 82, 109, 136],\n         [ 86, 112, 142],\n         [ 87, 113, 143]],\n \n        [[119, 141, 169],\n         [114, 136, 164],\n         [112, 136, 166],\n         ...,\n         [ 82, 106, 134],\n         [ 87, 111, 139],\n         [ 89, 113, 141]],\n \n        ...,\n \n        [[ 73, 175, 147],\n         [ 63, 163, 135],\n         [ 57, 151, 126],\n         ...,\n         [111, 162, 158],\n         [ 94, 148, 148],\n         [ 74, 130, 131]],\n \n        [[ 77, 194, 161],\n         [ 67, 181, 151],\n         [ 56, 166, 138],\n         ...,\n         [110, 165, 156],\n         [103, 161, 156],\n         [ 93, 153, 147]],\n \n        [[ 78, 204, 169],\n         [ 71, 194, 160],\n         [ 64, 180, 151],\n         ...,\n         [101, 159, 148],\n         [108, 168, 158],\n         [111, 171, 163]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_188_png.rf.9745ecfcfe31aa4fb165e008c2898deb.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7367134094238281, 'inference': 6.679296493530273, 'postprocess': 1.3997554779052734},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[117, 140, 178],\n         [110, 133, 171],\n         [ 99, 123, 159],\n         ...,\n         [ 98, 114, 143],\n         [106, 118, 152],\n         [108, 120, 154]],\n \n        [[110, 136, 172],\n         [112, 139, 173],\n         [108, 135, 169],\n         ...,\n         [ 78,  97, 124],\n         [101, 119, 148],\n         [117, 135, 164]],\n \n        [[107, 137, 166],\n         [111, 141, 168],\n         [105, 135, 162],\n         ...,\n         [ 63,  90, 110],\n         [ 94, 121, 142],\n         [116, 143, 164]],\n \n        ...,\n \n        [[120, 135, 167],\n         [104, 119, 151],\n         [ 73,  91, 122],\n         ...,\n         [ 90, 122, 151],\n         [105, 140, 173],\n         [113, 151, 183]],\n \n        [[123, 138, 170],\n         [109, 124, 156],\n         [ 74,  92, 123],\n         ...,\n         [106, 140, 169],\n         [ 98, 136, 168],\n         [106, 145, 177]],\n \n        [[145, 160, 192],\n         [124, 139, 171],\n         [ 80,  98, 129],\n         ...,\n         [126, 163, 191],\n         [103, 141, 173],\n         [ 99, 138, 170]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_18_png.rf.945f918b7846052452e031aec2028822.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.751495361328125, 'inference': 9.312629699707031, 'postprocess': 1.772165298461914},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 86, 147, 113],\n         [ 81, 145, 110],\n         [ 84, 149, 117],\n         ...,\n         [108, 135, 149],\n         [ 82, 111, 126],\n         [ 91, 120, 135]],\n \n        [[ 87, 145, 117],\n         [ 90, 149, 121],\n         [105, 166, 138],\n         ...,\n         [111, 137, 153],\n         [ 95, 123, 140],\n         [ 85, 113, 130]],\n \n        [[138, 187, 173],\n         [134, 185, 171],\n         [141, 194, 180],\n         ...,\n         [ 96, 122, 139],\n         [105, 132, 152],\n         [102, 129, 149]],\n \n        ...,\n \n        [[ 58,  63,  88],\n         [ 62,  69,  94],\n         [ 71,  82, 109],\n         ...,\n         [ 79,  96, 122],\n         [ 69,  86, 112],\n         [ 56,  73,  99]],\n \n        [[ 65,  69,  93],\n         [ 53,  60,  85],\n         [ 57,  68,  95],\n         ...,\n         [ 75,  92, 118],\n         [ 58,  75, 101],\n         [ 38,  55,  81]],\n \n        [[ 73,  77, 101],\n         [ 48,  53,  78],\n         [ 42,  53,  80],\n         ...,\n         [ 75,  92, 118],\n         [ 56,  73,  99],\n         [ 32,  49,  75]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_192_png.rf.aa94ed3b709c89860f1a52d0c02b975f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.253366470336914, 'inference': 6.483554840087891, 'postprocess': 1.2547969818115234},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 97, 113, 156],\n         [101, 117, 159],\n         [ 89, 105, 147],\n         ...,\n         [ 62,  80, 109],\n         [ 71,  87, 116],\n         [ 69,  85, 114]],\n \n        [[ 99, 115, 158],\n         [ 88, 104, 146],\n         [ 63,  79, 121],\n         ...,\n         [ 64,  82, 111],\n         [ 71,  87, 116],\n         [ 73,  89, 118]],\n \n        [[100, 116, 159],\n         [ 87, 103, 145],\n         [ 62,  80, 121],\n         ...,\n         [ 71,  89, 118],\n         [ 78,  94, 123],\n         [ 84, 100, 129]],\n \n        ...,\n \n        [[ 94, 112, 141],\n         [ 86, 104, 133],\n         [ 80, 101, 129],\n         ...,\n         [ 82, 104, 132],\n         [ 91, 112, 140],\n         [ 96, 117, 145]],\n \n        [[102, 118, 147],\n         [ 95, 113, 142],\n         [ 78,  99, 127],\n         ...,\n         [ 89, 111, 139],\n         [ 99, 120, 148],\n         [104, 125, 153]],\n \n        [[108, 124, 153],\n         [106, 124, 153],\n         [ 81,  99, 128],\n         ...,\n         [ 90, 112, 140],\n         [ 98, 119, 147],\n         [102, 123, 151]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_196_png.rf.83dca24251b5280cc1084e7fb722e8c9.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.268625259399414, 'inference': 6.650447845458984, 'postprocess': 1.2230873107910156},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 59, 106,  80],\n         [ 56, 102,  79],\n         [ 49,  95,  73],\n         ...,\n         [ 76,  98, 126],\n         [ 80, 104, 132],\n         [ 85, 109, 137]],\n \n        [[ 69, 115,  92],\n         [ 75, 121,  99],\n         [ 81, 126, 107],\n         ...,\n         [ 72,  94, 122],\n         [ 79, 103, 131],\n         [ 86, 110, 138]],\n \n        [[ 78, 120, 102],\n         [100, 142, 125],\n         [129, 170, 155],\n         ...,\n         [ 74,  96, 124],\n         [ 84, 108, 136],\n         [ 91, 115, 143]],\n \n        ...,\n \n        [[ 46,  50,  68],\n         [ 45,  49,  67],\n         [ 65,  71,  90],\n         ...,\n         [ 91, 111, 142],\n         [102, 123, 155],\n         [102, 123, 155]],\n \n        [[ 45,  49,  67],\n         [ 52,  56,  74],\n         [ 86,  92, 111],\n         ...,\n         [102, 123, 154],\n         [103, 124, 156],\n         [ 91, 112, 144]],\n \n        [[ 48,  52,  70],\n         [ 63,  67,  85],\n         [104, 110, 129],\n         ...,\n         [119, 140, 171],\n         [112, 133, 165],\n         [ 90, 111, 143]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_44_png.rf.a23efb05fa7977f94870fa3886fec577.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7481575012207031, 'inference': 6.535053253173828, 'postprocess': 1.2404918670654297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 95, 137, 136],\n         [ 85, 132, 130],\n         [ 69, 120, 116],\n         ...,\n         [ 77,  94, 113],\n         [ 82,  99, 118],\n         [ 87, 105, 122]],\n \n        [[128, 165, 163],\n         [121, 160, 158],\n         [104, 150, 144],\n         ...,\n         [ 61,  78,  97],\n         [ 68,  85, 104],\n         [ 75,  93, 110]],\n \n        [[171, 196, 192],\n         [163, 190, 186],\n         [149, 182, 175],\n         ...,\n         [ 37,  54,  73],\n         [ 39,  56,  75],\n         [ 42,  60,  77]],\n \n        ...,\n \n        [[ 34,  56,  68],\n         [ 53,  81,  88],\n         [ 81, 114, 117],\n         ...,\n         [ 79, 100, 122],\n         [ 87, 108, 130],\n         [ 91, 112, 134]],\n \n        [[ 47,  72,  82],\n         [ 61,  90,  97],\n         [ 77, 112, 115],\n         ...,\n         [ 67,  88, 110],\n         [ 76,  97, 119],\n         [ 90, 111, 133]],\n \n        [[ 67,  94, 104],\n         [ 67,  96, 103],\n         [ 65, 103, 103],\n         ...,\n         [ 61,  82, 104],\n         [ 66,  87, 109],\n         [ 83, 104, 126]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_53_png.rf.9a18f731eaaf83ab30927ca37379e142.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7636547088623047, 'inference': 6.515979766845703, 'postprocess': 1.2459754943847656},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 91, 104, 136],\n         [ 85,  99, 128],\n         [ 82,  93, 123],\n         ...,\n         [ 69,  82, 108],\n         [ 87, 102, 128],\n         [102, 120, 143]],\n \n        [[ 84,  97, 129],\n         [ 78,  92, 121],\n         [ 75,  87, 115],\n         ...,\n         [ 64,  78, 106],\n         [ 78,  95, 121],\n         [ 93, 114, 136]],\n \n        [[ 82,  98, 127],\n         [ 77,  91, 120],\n         [ 73,  85, 113],\n         ...,\n         [ 58,  75, 102],\n         [ 59,  80, 107],\n         [ 67,  91, 115]],\n \n        ...,\n \n        [[ 84, 105, 126],\n         [ 84, 105, 126],\n         [ 72,  93, 114],\n         ...,\n         [101, 134, 143],\n         [ 76, 109, 118],\n         [ 57,  93, 101]],\n \n        [[ 80, 100, 117],\n         [ 75,  95, 112],\n         [ 68,  88, 106],\n         ...,\n         [103, 137, 150],\n         [ 76, 110, 123],\n         [ 56,  91, 104]],\n \n        [[ 88, 109, 124],\n         [ 79, 100, 115],\n         [ 72,  90, 107],\n         ...,\n         [115, 150, 164],\n         [ 81, 116, 130],\n         [ 58,  93, 107]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_75_png.rf.23155c3496205e4094459310f857b293.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3175010681152344, 'inference': 6.314754486083984, 'postprocess': 1.2252330780029297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[127, 139, 167],\n         [138, 150, 178],\n         [132, 147, 173],\n         ...,\n         [ 40,  62,  90],\n         [ 37,  57,  92],\n         [ 58,  80, 116]],\n \n        [[ 90, 101, 131],\n         [ 94, 108, 136],\n         [ 93, 107, 135],\n         ...,\n         [ 72,  92, 117],\n         [ 56,  76, 107],\n         [ 43,  64,  96]],\n \n        [[ 44,  57,  89],\n         [ 42,  58,  87],\n         [ 43,  59,  88],\n         ...,\n         [ 90, 109, 124],\n         [ 66,  85, 106],\n         [ 35,  53,  76]],\n \n        ...,\n \n        [[ 99, 124, 156],\n         [ 96, 121, 153],\n         [ 90, 115, 147],\n         ...,\n         [ 66,  82,  99],\n         [ 47,  60,  76],\n         [ 40,  53,  69]],\n \n        [[ 93, 118, 150],\n         [ 88, 113, 145],\n         [ 79, 106, 140],\n         ...,\n         [ 79,  97, 114],\n         [ 63,  78,  94],\n         [ 60,  75,  91]],\n \n        [[ 87, 112, 144],\n         [ 85, 110, 142],\n         [ 78, 105, 139],\n         ...,\n         [ 93, 111, 128],\n         [ 84, 100, 116],\n         [ 84,  99, 115]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_78_png.rf.3ae8d098be942fa2848c56989bdeda92.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2421607971191406, 'inference': 6.565093994140625, 'postprocess': 1.2226104736328125},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 67,  81, 104],\n         [ 60,  74,  97],\n         [ 49,  64,  90],\n         ...,\n         [ 93, 121, 155],\n         [ 74, 105, 138],\n         [ 67,  98, 131]],\n \n        [[ 63,  77, 100],\n         [ 54,  68,  91],\n         [ 38,  53,  79],\n         ...,\n         [ 93, 121, 155],\n         [ 76, 107, 140],\n         [ 70, 101, 134]],\n \n        [[ 67,  81, 104],\n         [ 58,  72,  95],\n         [ 42,  57,  83],\n         ...,\n         [ 90, 118, 152],\n         [ 76, 107, 140],\n         [ 73, 104, 137]],\n \n        ...,\n \n        [[ 87, 115, 145],\n         [ 76, 104, 134],\n         [ 77, 105, 135],\n         ...,\n         [111, 136, 168],\n         [ 88, 115, 142],\n         [ 73, 101, 125]],\n \n        [[ 83, 110, 137],\n         [ 80, 107, 134],\n         [ 85, 112, 139],\n         ...,\n         [108, 130, 165],\n         [ 84, 108, 138],\n         [ 71,  96, 122]],\n \n        [[ 81, 108, 135],\n         [ 83, 110, 137],\n         [ 92, 119, 146],\n         ...,\n         [100, 122, 158],\n         [ 81, 102, 133],\n         [ 72,  94, 122]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_82_png.rf.73e51476d5cdf1f15e0006114582c3a2.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3060569763183594, 'inference': 6.439685821533203, 'postprocess': 1.2116432189941406},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[108, 138, 167],\n         [109, 139, 168],\n         [109, 139, 168],\n         ...,\n         [109, 129, 160],\n         [119, 139, 170],\n         [120, 140, 171]],\n \n        [[100, 130, 159],\n         [102, 132, 161],\n         [105, 135, 164],\n         ...,\n         [102, 122, 153],\n         [114, 134, 165],\n         [120, 140, 171]],\n \n        [[ 94, 123, 154],\n         [ 98, 127, 158],\n         [103, 132, 163],\n         ...,\n         [ 90, 110, 141],\n         [ 94, 114, 145],\n         [ 97, 117, 148]],\n \n        ...,\n \n        [[ 73, 157, 129],\n         [ 79, 163, 135],\n         [ 86, 168, 145],\n         ...,\n         [ 12,  33,  41],\n         [ 21,  41,  52],\n         [ 23,  45,  56]],\n \n        [[ 84, 159, 138],\n         [ 88, 162, 144],\n         [ 90, 162, 149],\n         ...,\n         [ 44,  64,  75],\n         [ 52,  74,  86],\n         [ 45,  69,  81]],\n \n        [[105, 176, 160],\n         [105, 175, 162],\n         [100, 169, 158],\n         ...,\n         [ 67,  89, 100],\n         [ 83, 104, 119],\n         [ 67,  90, 105]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_89_png.rf.21b9d634f712eb1263ae6891e20da1a1.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2204647064208984, 'inference': 6.676673889160156, 'postprocess': 1.1906623840332031},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 67,  88, 116],\n         [ 76,  97, 125],\n         [ 97, 118, 149],\n         ...,\n         [ 83, 102, 137],\n         [102, 127, 161],\n         [117, 144, 178]],\n \n        [[ 71,  92, 120],\n         [ 68,  89, 117],\n         [ 76,  97, 128],\n         ...,\n         [ 90, 109, 144],\n         [105, 127, 162],\n         [115, 142, 176]],\n \n        [[ 76,  97, 125],\n         [ 63,  84, 112],\n         [ 61,  82, 113],\n         ...,\n         [ 85, 104, 137],\n         [102, 125, 157],\n         [116, 141, 173]],\n \n        ...,\n \n        [[ 47,  63,  92],\n         [ 59,  75, 104],\n         [ 71,  87, 116],\n         ...,\n         [100, 121, 148],\n         [110, 133, 159],\n         [100, 123, 149]],\n \n        [[ 59,  75, 104],\n         [ 69,  85, 114],\n         [ 80,  96, 125],\n         ...,\n         [ 78,  99, 127],\n         [ 81, 103, 131],\n         [ 78, 100, 128]],\n \n        [[ 86, 102, 131],\n         [ 90, 106, 135],\n         [ 92, 108, 137],\n         ...,\n         [ 60,  81, 109],\n         [ 67,  88, 119],\n         [ 80, 101, 132]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_95_png.rf.143171f3e7e6d6f9540fef48d833d8af.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2493133544921875, 'inference': 6.557703018188477, 'postprocess': 1.2412071228027344},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 68, 103,  99],\n         [ 55,  90,  86],\n         [ 54,  87,  83],\n         ...,\n         [ 49,  61,  79],\n         [ 65,  79,  97],\n         [ 69,  83, 101]],\n \n        [[ 74, 109, 105],\n         [ 67, 102,  98],\n         [ 72, 107, 103],\n         ...,\n         [ 64,  76,  94],\n         [ 67,  81,  99],\n         [ 65,  79,  97]],\n \n        [[ 89, 126, 122],\n         [ 88, 125, 121],\n         [ 99, 136, 132],\n         ...,\n         [ 78,  90, 108],\n         [ 65,  79,  97],\n         [ 58,  72,  90]],\n \n        ...,\n \n        [[ 52,  59,  79],\n         [ 54,  61,  81],\n         [ 67,  76,  96],\n         ...,\n         [ 97, 109, 133],\n         [ 99, 112, 138],\n         [102, 115, 141]],\n \n        [[ 55,  61,  84],\n         [ 49,  55,  78],\n         [ 51,  59,  82],\n         ...,\n         [ 87,  98, 125],\n         [ 93, 104, 131],\n         [ 98, 109, 136]],\n \n        [[ 57,  63,  86],\n         [ 55,  61,  84],\n         [ 41,  49,  72],\n         ...,\n         [ 86,  97, 124],\n         [ 88,  99, 126],\n         [ 91, 102, 129]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_09_png.rf.c47d7daf1c117acb79a82f0b16edbfae.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.25885009765625, 'inference': 6.527423858642578, 'postprocess': 1.2583732604980469},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[116, 122, 151],\n         [ 98, 104, 133],\n         [ 78,  86, 116],\n         ...,\n         [ 47, 115,  98],\n         [ 97, 159, 147],\n         [113, 173, 162]],\n \n        [[111, 117, 146],\n         [ 97, 105, 134],\n         [ 83,  91, 120],\n         ...,\n         [ 33, 100,  85],\n         [ 79, 141, 129],\n         [112, 172, 162]],\n \n        [[ 91, 100, 127],\n         [ 87,  96, 123],\n         [ 81,  90, 117],\n         ...,\n         [ 40, 103,  93],\n         [ 67, 127, 119],\n         [ 93, 152, 144]],\n \n        ...,\n \n        [[114, 133, 146],\n         [ 98, 118, 129],\n         [ 83, 106, 114],\n         ...,\n         [ 55,  91,  99],\n         [ 55,  87, 100],\n         [ 42,  74,  87]],\n \n        [[ 99, 130, 127],\n         [ 97, 130, 126],\n         [100, 136, 130],\n         ...,\n         [ 85, 103, 120],\n         [ 84,  98, 120],\n         [ 79,  93, 115]],\n \n        [[ 85, 122, 112],\n         [108, 148, 136],\n         [140, 180, 168],\n         ...,\n         [103, 114, 136],\n         [108, 113, 138],\n         [116, 120, 148]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_11_png.rf.c764badc553ebd7c9893e0be843e134f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2786388397216797, 'inference': 6.464719772338867, 'postprocess': 1.2276172637939453},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 73,  94,  91],\n         [ 70,  91,  89],\n         [ 72,  87,  89],\n         ...,\n         [ 68,  82, 101],\n         [ 72,  87, 106],\n         [ 81,  96, 115]],\n \n        [[ 75,  97,  95],\n         [ 70,  91,  92],\n         [ 68,  85,  88],\n         ...,\n         [ 71,  85, 104],\n         [ 69,  84, 103],\n         [ 73,  88, 107]],\n \n        [[ 83, 111, 112],\n         [ 75, 100, 104],\n         [ 69,  88,  95],\n         ...,\n         [ 79,  93, 112],\n         [ 73,  88, 107],\n         [ 73,  88, 107]],\n \n        ...,\n \n        [[ 48,  85,  81],\n         [ 34,  74,  69],\n         [ 33,  74,  69],\n         ...,\n         [ 71,  82, 102],\n         [ 71,  85, 103],\n         [ 72,  86, 104]],\n \n        [[ 68, 110, 103],\n         [ 59, 103,  96],\n         [ 64, 111, 103],\n         ...,\n         [ 68,  79,  99],\n         [ 68,  82, 100],\n         [ 71,  85, 103]],\n \n        [[ 74, 118, 111],\n         [ 81, 128, 120],\n         [ 95, 142, 134],\n         ...,\n         [ 67,  78,  98],\n         [ 66,  80,  98],\n         [ 69,  83, 101]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_16_png.rf.a64a46de9434e21c7455d71d209369dd.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3473033905029297, 'inference': 6.5460205078125, 'postprocess': 1.2466907501220703},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 93, 107, 126],\n         [ 98, 112, 131],\n         [ 88, 101, 123],\n         ...,\n         [ 78,  89, 111],\n         [ 69,  80, 102],\n         [ 73,  84, 106]],\n \n        [[ 96, 110, 129],\n         [ 96, 110, 129],\n         [ 82,  95, 117],\n         ...,\n         [ 75,  86, 108],\n         [ 68,  79, 101],\n         [ 73,  84, 106]],\n \n        [[ 91, 104, 126],\n         [ 90, 103, 125],\n         [ 77,  90, 112],\n         ...,\n         [ 74,  85, 107],\n         [ 69,  80, 102],\n         [ 73,  84, 106]],\n \n        ...,\n \n        [[ 74,  88, 107],\n         [ 75,  89, 108],\n         [ 75,  89, 108],\n         ...,\n         [ 71,  85, 104],\n         [ 69,  83, 102],\n         [ 67,  81, 100]],\n \n        [[ 73,  87, 106],\n         [ 74,  88, 107],\n         [ 74,  88, 107],\n         ...,\n         [ 71,  85, 104],\n         [ 73,  87, 106],\n         [ 75,  89, 108]],\n \n        [[ 75,  89, 108],\n         [ 76,  90, 109],\n         [ 75,  89, 108],\n         ...,\n         [ 72,  86, 105],\n         [ 78,  92, 111],\n         [ 83,  97, 116]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_18_png.rf.b65f0023274748b9a404e347de55d35d.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3341903686523438, 'inference': 6.387948989868164, 'postprocess': 1.4235973358154297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 79,  95, 112],\n         [ 78,  94, 111],\n         [ 80,  95, 111],\n         ...,\n         [ 74,  83,  96],\n         [ 73,  81,  94],\n         [ 73,  81,  94]],\n \n        [[ 64,  80,  97],\n         [ 63,  79,  95],\n         [ 66,  81,  97],\n         ...,\n         [ 73,  81,  94],\n         [ 70,  78,  91],\n         [ 69,  77,  90]],\n \n        [[ 58,  73,  89],\n         [ 53,  68,  84],\n         [ 55,  68,  84],\n         ...,\n         [ 80,  86,  97],\n         [ 75,  81,  92],\n         [ 73,  79,  90]],\n \n        ...,\n \n        [[ 20,  21,  42],\n         [ 18,  19,  40],\n         [ 20,  21,  42],\n         ...,\n         [ 80,  88, 105],\n         [ 81,  89, 106],\n         [ 83,  91, 108]],\n \n        [[ 55,  52,  77],\n         [ 55,  53,  75],\n         [ 52,  50,  72],\n         ...,\n         [ 81,  88, 105],\n         [ 80,  87, 104],\n         [ 79,  86, 103]],\n \n        [[ 71,  68,  93],\n         [ 72,  69,  94],\n         [ 66,  64,  86],\n         ...,\n         [ 81,  88, 105],\n         [ 77,  84, 101],\n         [ 75,  82,  99]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_25_png.rf.5ea0dd56e58ae897e3fbb1565808c72c.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3773441314697266, 'inference': 6.484270095825195, 'postprocess': 1.2445449829101562},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 48,  46,  52],\n         [ 61,  61,  67],\n         [ 74,  76,  86],\n         ...,\n         [ 67,  80, 102],\n         [ 73,  86, 108],\n         [ 77,  90, 112]],\n \n        [[ 41,  39,  45],\n         [ 52,  52,  58],\n         [ 64,  66,  76],\n         ...,\n         [ 70,  83, 105],\n         [ 74,  87, 109],\n         [ 75,  88, 110]],\n \n        [[ 37,  33,  39],\n         [ 45,  43,  49],\n         [ 51,  52,  62],\n         ...,\n         [ 78,  91, 113],\n         [ 78,  91, 113],\n         [ 74,  87, 109]],\n \n        ...,\n \n        [[ 35,  37,  47],\n         [ 39,  44,  53],\n         [ 38,  43,  52],\n         ...,\n         [ 78,  89, 111],\n         [ 72,  83, 105],\n         [ 66,  77,  99]],\n \n        [[ 32,  35,  43],\n         [ 31,  37,  44],\n         [ 33,  38,  47],\n         ...,\n         [ 78,  86, 109],\n         [ 73,  81, 104],\n         [ 69,  77, 100]],\n \n        [[ 34,  38,  43],\n         [ 33,  39,  44],\n         [ 39,  44,  53],\n         ...,\n         [ 69,  77, 100],\n         [ 70,  76,  99],\n         [ 71,  77, 100]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_48_png.rf.a9d3e2d8f0ae863c1589fcd8f21c9c3c.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2638568878173828, 'inference': 6.303548812866211, 'postprocess': 1.2125968933105469}]"},"metadata":{}}],"execution_count":14}]}