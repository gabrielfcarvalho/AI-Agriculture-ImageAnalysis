{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1Elio6XF-zSHRQ7AKVjBpTkMCoVXHIrgp","authorship_tag":"ABX9TyPjEMJfHWP2oA+gO5uj2lqM"},"gpuClass":"standard","accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[{"sourceId":7705447,"sourceType":"datasetVersion","datasetId":4498585}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/gabrielfcarvalho/yolov8-sorghum-detection?scriptVersionId=164408393\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **GPU**","metadata":{"id":"kqZFu7lcAdRq"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"Czu63cCsAftH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682715944055,"user_tz":180,"elapsed":1074,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"644275e4-3652-4081-998c-c404c47c627d","execution":{"iopub.status.busy":"2024-02-26T15:39:22.973054Z","iopub.execute_input":"2024-02-26T15:39:22.973749Z","iopub.status.idle":"2024-02-26T15:39:23.967419Z","shell.execute_reply.started":"2024-02-26T15:39:22.973719Z","shell.execute_reply":"2024-02-26T15:39:23.966217Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Mon Feb 26 15:39:23 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0              26W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **Installing Ultralytics**","metadata":{"id":"Ky3lxBDcAjLj"}},{"cell_type":"code","source":"! pip install ultralytics","metadata":{"id":"NLKlVnA-An2Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685044683387,"user_tz":180,"elapsed":5659,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"761596ff-95fb-4c92-c8ad-b62ded3fcd73","execution":{"iopub.status.busy":"2024-02-26T15:39:43.971814Z","iopub.execute_input":"2024-02-26T15:39:43.972521Z","iopub.status.idle":"2024-02-26T15:39:58.212297Z","shell.execute_reply.started":"2024-02-26T15:39:43.972485Z","shell.execute_reply":"2024-02-26T15:39:58.21139Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.1.18-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.4)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nCollecting thop>=0.1.1 (from ultralytics)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.24.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2023.12.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.1.18-py3-none-any.whl (716 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.0/716.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.1.18\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# **Modifying yaml file**","metadata":{}},{"cell_type":"code","source":"%cp /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/data.yaml /kaggle/working/data.yaml","metadata":{"execution":{"iopub.status.busy":"2024-02-26T15:52:06.160903Z","iopub.execute_input":"2024-02-26T15:52:06.161736Z","iopub.status.idle":"2024-02-26T15:52:07.120677Z","shell.execute_reply.started":"2024-02-26T15:52:06.161702Z","shell.execute_reply":"2024-02-26T15:52:07.119445Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Let's create a new data.yaml content with the updated paths as requested by the user.\n\nnew_data_yaml_content = \"\"\"train: /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/train/images\nval: /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/valid/images\ntest: /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images\n\nnc: 1\nnames: ['c']\n\"\"\"\n\n# Save the content to a new file\nyaml_file_path = '/kaggle/working/data.yaml'\nwith open(yaml_file_path, 'w') as file:\n    file.write(new_data_yaml_content)","metadata":{"execution":{"iopub.status.busy":"2024-02-26T15:52:28.995372Z","iopub.execute_input":"2024-02-26T15:52:28.99577Z","iopub.status.idle":"2024-02-26T15:52:29.00229Z","shell.execute_reply.started":"2024-02-26T15:52:28.995737Z","shell.execute_reply":"2024-02-26T15:52:29.001386Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# **Training Model**","metadata":{"id":"QEtDb-LBHD2I"}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\n\nos.environ['WANDB_DISABLED'] = 'true'\n\n# Load a model\nmodel = YOLO('yolov8s.pt')  # load a pretrained model (recommended for training)\n\n# Train the model\nresults = model.train(data='/kaggle/working/data.yaml', epochs=100, imgsz=416, cache=True, device=0, patience=0, batch=16)","metadata":{"id":"V8leWnJZHFxF","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1682709915619,"user_tz":180,"elapsed":482660,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"e7066e45-a631-427e-fab9-6f997efe24b1","execution":{"iopub.status.busy":"2024-02-26T15:59:41.443445Z","iopub.execute_input":"2024-02-26T15:59:41.444385Z","iopub.status.idle":"2024-02-26T16:09:09.151376Z","shell.execute_reply.started":"2024-02-26T15:59:41.444329Z","shell.execute_reply":"2024-02-26T16:09:09.149586Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.18 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/kaggle/working/data.yaml, epochs=100, time=None, patience=0, batch=16, imgsz=416, save=True, save_period=-1, cache=True, device=0, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train4\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \nModel summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n\nTransferred 349/355 items from pretrained weights\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train4', view at http://localhost:6006/\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240226_160101-oipqij17</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gabrielfc2102/YOLOv8/runs/oipqij17' target=\"_blank\">train4</a></strong> to <a href='https://wandb.ai/gabrielfc2102/YOLOv8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gabrielfc2102/YOLOv8' target=\"_blank\">https://wandb.ai/gabrielfc2102/YOLOv8</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gabrielfc2102/YOLOv8/runs/oipqij17' target=\"_blank\">https://wandb.ai/gabrielfc2102/YOLOv8/runs/oipqij17</a>"},"metadata":{}},{"name":"stdout","text":"Freezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/train/labels... 723 images, 0 backgrounds, 0 corrupt: 100%|██████████| 723/723 [00:02<00:00, 327.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/train is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.3GB True): 100%|██████████| 723/723 [00:01<00:00, 381.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/valid/labels... 25 images, 0 backgrounds, 0 corrupt: 100%|██████████| 25/25 [00:00<00:00, 231.92it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/valid is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB True): 100%|██████████| 25/25 [00:00<00:00, 187.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/train4/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 416 train, 416 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train4\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      1/100      1.98G      2.064      1.807      1.701         39        416: 100%|██████████| 46/46 [00:12<00:00,  3.56it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:01<00:00,  1.31s/it]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.483      0.823      0.549      0.276\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      2/100      2.09G      1.545      1.096      1.304         39        416: 100%|██████████| 46/46 [00:07<00:00,  5.84it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.323      0.637      0.299      0.135\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      3/100      2.07G       1.52      1.035      1.313         47        416: 100%|██████████| 46/46 [00:07<00:00,  5.91it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.62      0.575      0.586      0.284\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      4/100      2.06G      1.485      1.012      1.288         60        416: 100%|██████████| 46/46 [00:07<00:00,  5.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.42it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.69      0.613      0.688      0.341\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      5/100      2.05G      1.482      1.015      1.288         45        416: 100%|██████████| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.18it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.779      0.684      0.814       0.43\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      6/100      2.06G      1.453     0.9796      1.267         58        416: 100%|██████████| 46/46 [00:07<00:00,  5.90it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.46it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.779      0.745      0.844       0.47\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      7/100      2.05G       1.44     0.9839      1.268         43        416: 100%|██████████| 46/46 [00:07<00:00,  6.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.51it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.81      0.806      0.874      0.463\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      8/100      2.06G      1.426      0.944      1.266         26        416: 100%|██████████| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.78it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.746      0.804      0.849      0.476\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      9/100      2.06G       1.41     0.9258      1.251         37        416: 100%|██████████| 46/46 [00:07<00:00,  5.92it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.719      0.764      0.783      0.421\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     10/100      2.06G      1.407     0.9257      1.236         34        416: 100%|██████████| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.829      0.779      0.892      0.508\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     11/100      2.06G      1.384     0.9018      1.231         41        416: 100%|██████████| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.774      0.774      0.861      0.464\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     12/100      2.06G      1.367     0.8906       1.22         43        416: 100%|██████████| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.763       0.84      0.839      0.429\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     13/100      2.05G      1.381     0.9217       1.24         36        416: 100%|██████████| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.82      0.703      0.846      0.478\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     14/100      2.06G      1.356      0.899      1.216         36        416: 100%|██████████| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.786      0.781      0.833      0.467\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     15/100      1.97G      1.361     0.9016      1.225         24        416: 100%|██████████| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.28it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.771      0.788      0.838      0.481\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     16/100      2.04G      1.334     0.8717      1.196         29        416: 100%|██████████| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.60it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.683      0.868      0.818      0.452\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     17/100      1.97G      1.338     0.9025      1.208         32        416: 100%|██████████| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.35it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.838      0.825      0.882      0.508\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     18/100      2.06G       1.34     0.8637      1.209         27        416: 100%|██████████| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.99it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.86      0.811      0.891      0.488\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     19/100      2.06G      1.314     0.8522      1.197         30        416: 100%|██████████| 46/46 [00:07<00:00,  6.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.34it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.761      0.751      0.804      0.438\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     20/100      2.06G      1.341     0.8726      1.213         45        416: 100%|██████████| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.11it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.814      0.797      0.868      0.474\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     21/100      2.04G      1.316     0.8503        1.2         38        416: 100%|██████████| 46/46 [00:07<00:00,  5.98it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.776      0.802      0.862      0.476\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     22/100      2.06G       1.32     0.8265      1.206         36        416: 100%|██████████| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.826      0.854      0.897      0.483\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     23/100      2.05G      1.323     0.8497      1.199         51        416: 100%|██████████| 46/46 [00:07<00:00,  5.99it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.44it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.829       0.83       0.88      0.517\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     24/100      2.06G      1.328     0.8226      1.198         41        416: 100%|██████████| 46/46 [00:07<00:00,  6.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.68it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.791      0.868      0.888      0.504\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     25/100      1.97G      1.302     0.8158      1.191         42        416: 100%|██████████| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.83it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.84       0.79      0.883      0.518\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     26/100      2.05G      1.312     0.8227      1.192         39        416: 100%|██████████| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.31it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.804      0.811      0.875       0.49\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     27/100      1.97G      1.304     0.8459      1.192         33        416: 100%|██████████| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.09it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.864       0.83      0.894      0.516\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     28/100      2.05G      1.288     0.8058      1.184         47        416: 100%|██████████| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.827      0.811        0.9      0.503\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     29/100      1.98G      1.277     0.7917      1.168         41        416: 100%|██████████| 46/46 [00:07<00:00,  5.97it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.62it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.803      0.811      0.853      0.475\n","output_type":"stream"},{"name":"stderr","text":"\n     32/100      2.06G      1.301     0.8025      1.181         33        416: 100%|██████████| 46/46 [00:07<00:00,  6.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.88it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.845      0.792      0.882      0.522\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     33/100      1.97G      1.275     0.7758      1.166         54        416: 100%|██████████| 46/46 [00:07<00:00,  5.95it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.57it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.832      0.844       0.87       0.53\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     34/100      2.05G      1.261     0.7716      1.167         42        416: 100%|██████████| 46/46 [00:07<00:00,  6.03it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.787      0.855       0.88      0.513\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     35/100      2.06G      1.263     0.7687      1.158         38        416: 100%|██████████| 46/46 [00:07<00:00,  6.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.48it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.863      0.821      0.892        0.5\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     36/100      1.98G      1.248      0.771      1.152         38        416: 100%|██████████| 46/46 [00:07<00:00,  6.05it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.824      0.825      0.869      0.499\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     37/100      2.06G      1.266     0.7632       1.16         50        416: 100%|██████████| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.871      0.821      0.908      0.513\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     38/100      2.06G      1.263     0.7657      1.164         24        416: 100%|██████████| 46/46 [00:07<00:00,  6.04it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.33it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.863      0.821      0.885       0.48\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     39/100      2.06G      1.246     0.7602      1.156         29        416: 100%|██████████| 46/46 [00:07<00:00,  5.96it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.40it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.857      0.868      0.894      0.526\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     40/100      1.98G      1.229     0.7366      1.141         38        416: 100%|██████████| 46/46 [00:07<00:00,  6.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.86      0.816      0.887      0.503\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     41/100      2.05G      1.239     0.7506      1.151         33        416: 100%|██████████| 46/46 [00:07<00:00,  5.94it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.41it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.765      0.854       0.87      0.481\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     42/100      2.05G      1.223     0.7336      1.136         59        416: 100%|██████████| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  5.73it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.876      0.867      0.911      0.531\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     43/100      1.97G      1.228     0.7234       1.15         52        416: 100%|██████████| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.72it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.851      0.838      0.896      0.515\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     44/100      2.06G      1.228     0.7308      1.155         38        416: 100%|██████████| 46/46 [00:07<00:00,  6.00it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  4.63it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212       0.84      0.802      0.864      0.478\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     45/100      1.97G      1.213     0.7129      1.136         49        416: 100%|██████████| 46/46 [00:07<00:00,  6.02it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.49it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.809      0.816      0.852      0.478\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     46/100      1.97G      1.214     0.7323       1.14         52        416: 100%|██████████| 46/46 [00:07<00:00,  6.01it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 1/1 [00:00<00:00,  6.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all         25        212      0.835      0.769       0.87      0.496\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     47/100      1.98G      1.204     0.7301      1.136        223        416:  63%|██████▎   | 29/46 [00:04<00:02,  5.87it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov8s.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/data.yaml\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m416\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:644\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:208\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/trainer.py:376\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp):\n\u001b[1;32m    375\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_batch(batch)\n\u001b[0;32m--> 376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m world_size\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:82\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03mForward pass of the model on a single scale. Wrapper for `_forward_once` method.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): The output of the network.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:261\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[0;34m(self, batch, preds)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[1;32m    260\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m preds\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/utils/loss.py:214\u001b[0m, in \u001b[0;36mv8DetectionLoss.__call__\u001b[0;34m(self, preds, batch)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Targets\u001b[39;00m\n\u001b[1;32m    213\u001b[0m targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 214\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m gt_labels, gt_bboxes \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39msplit((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m), \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# cls, xyxy\u001b[39;00m\n\u001b[1;32m    216\u001b[0m mask_gt \u001b[38;5;241m=\u001b[39m gt_bboxes\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mgt_(\u001b[38;5;241m0\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/utils/loss.py:184\u001b[0m, in \u001b[0;36mv8DetectionLoss.preprocess\u001b[0;34m(self, targets, batch_size, scale_tensor)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n:\n\u001b[1;32m    183\u001b[0m             out[j, :n] \u001b[38;5;241m=\u001b[39m targets[matches, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 184\u001b[0m     out[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mxywh2xyxy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":12},{"cell_type":"markdown","source":"# **Model Validation**","metadata":{"id":"7044GQnbgmoN"}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('/kaggle/working/runs/detect/train4/weights/best.pt')  # load a custom model\nmetrics = model.val(split = 'test')  # no arguments needed, dataset and settings remembered","metadata":{"id":"KsnaEQ9TkLEo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682709247038,"user_tz":180,"elapsed":49216,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"fa4eb179-f72c-4e0e-9b8c-1a7e38e4411f","execution":{"iopub.status.busy":"2024-02-26T16:09:34.61255Z","iopub.execute_input":"2024-02-26T16:09:34.613255Z","iopub.status.idle":"2024-02-26T16:09:42.402252Z","shell.execute_reply.started":"2024-02-26T16:09:34.6132Z","shell.execute_reply":"2024-02-26T16:09:42.400949Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.1.18 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla P100-PCIE-16GB, 16276MiB)\nModel summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/labels... 32 images, 0 backgrounds, 0 corrupt: 100%|██████████| 32/32 [00:00<00:00, 335.11it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mWARNING ⚠️ Cache directory /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test is not writeable, cache not saved.\n","output_type":"stream"},{"name":"stderr","text":"\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:02<00:00,  1.20s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all         32        257      0.792      0.846      0.864      0.509\nSpeed: 0.1ms preprocess, 7.4ms inference, 0.0ms loss, 1.4ms postprocess per image\nResults saved to \u001b[1mruns/detect/val\u001b[0m\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"#metrics.box.map    # map50-95\nmetrics.box.map50  # map50\n#metrics.box.map75  # map75\n#metrics.box.maps   # a list contains map50-95 of each category","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gGBhc4O7W8yY","executionInfo":{"status":"ok","timestamp":1682708984415,"user_tz":180,"elapsed":3,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"943ca8e7-3e88-4fe4-c249-764999ae4c69"},"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":["0.8554853986895027"]},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"# **Testing the model in the images**","metadata":{"id":"J17quNpbrUUo"}},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('/kaggle/working/runs/detect/train4/weights/best.pt')  # load a custom model\nmodel.predict('/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images', save=True, imgsz=416, conf=0.5, show_labels=True, show_conf=True)","metadata":{"id":"cYqJg12UrY1A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685044715042,"user_tz":180,"elapsed":1955,"user":{"displayName":"Gabriel Fernandes","userId":"00649530448706661079"}},"outputId":"34d6a666-6e16-4abb-85b6-5e418653a8b2","execution":{"iopub.status.busy":"2024-02-26T16:10:39.96764Z","iopub.execute_input":"2024-02-26T16:10:39.968423Z","iopub.status.idle":"2024-02-26T16:10:41.086447Z","shell.execute_reply.started":"2024-02-26T16:10:39.968356Z","shell.execute_reply":"2024-02-26T16:10:41.085329Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nimage 1/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_17_png.rf.2cbfe9d703e1046a6bd252360814d983.jpg: 416x416 9 cs, 6.7ms\nimage 2/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_20_png.rf.b42eb18de55a877b0e327be4c2596bf9.jpg: 416x416 9 cs, 8.9ms\nimage 3/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_25_png.rf.c782ccc3452d9c73651d90080c7fa248.jpg: 416x416 9 cs, 6.7ms\nimage 4/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_28_png.rf.9749fcf39aea7932df9a36f8645b7381.jpg: 416x416 12 cs, 6.4ms\nimage 5/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_33_png.rf.8206f2b80887a56d425abdfcaf22ab49.jpg: 416x416 12 cs, 6.5ms\nimage 6/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_35_png.rf.d5391a0424a348be3bef0010be20801f.jpg: 416x416 9 cs, 6.4ms\nimage 7/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_48_png.rf.fc2f106cbecb2aca31605850989bfd78.jpg: 416x416 9 cs, 6.4ms\nimage 8/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_105_png.rf.654dfc59c220ab8e879d185ae947fd0e.jpg: 416x416 8 cs, 6.4ms\nimage 9/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_121_png.rf.f57d5de666f070bc561ff20c721c8e8f.jpg: 416x416 9 cs, 6.3ms\nimage 10/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_122_png.rf.e0a857770d48399eb4e8ed75f0469b9e.jpg: 416x416 9 cs, 6.4ms\nimage 11/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_124_png.rf.880cf55a745bd87522d9ac1cf09334be.jpg: 416x416 7 cs, 6.5ms\nimage 12/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_12_png.rf.896d9610e8b3109bb4d498202c89c2de.jpg: 416x416 5 cs, 6.3ms\nimage 13/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_134_png.rf.41ef2a70982eb3781162c1f9428e7a34.jpg: 416x416 7 cs, 6.6ms\nimage 14/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_160_png.rf.3e1a825bd9b8af6aedb164f527c1b412.jpg: 416x416 18 cs, 6.8ms\nimage 15/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_166_png.rf.9cdd602f0662e345ab143cb4ebc58edb.jpg: 416x416 11 cs, 6.7ms\nimage 16/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_188_png.rf.9745ecfcfe31aa4fb165e008c2898deb.jpg: 416x416 9 cs, 6.7ms\nimage 17/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_18_png.rf.945f918b7846052452e031aec2028822.jpg: 416x416 14 cs, 9.3ms\nimage 18/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_192_png.rf.aa94ed3b709c89860f1a52d0c02b975f.jpg: 416x416 16 cs, 6.5ms\nimage 19/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_196_png.rf.83dca24251b5280cc1084e7fb722e8c9.jpg: 416x416 9 cs, 6.7ms\nimage 20/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_44_png.rf.a23efb05fa7977f94870fa3886fec577.jpg: 416x416 8 cs, 6.5ms\nimage 21/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_53_png.rf.9a18f731eaaf83ab30927ca37379e142.jpg: 416x416 8 cs, 6.5ms\nimage 22/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_75_png.rf.23155c3496205e4094459310f857b293.jpg: 416x416 10 cs, 6.3ms\nimage 23/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_78_png.rf.3ae8d098be942fa2848c56989bdeda92.jpg: 416x416 12 cs, 6.6ms\nimage 24/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_82_png.rf.73e51476d5cdf1f15e0006114582c3a2.jpg: 416x416 5 cs, 6.4ms\nimage 25/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_89_png.rf.21b9d634f712eb1263ae6891e20da1a1.jpg: 416x416 5 cs, 6.7ms\nimage 26/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_95_png.rf.143171f3e7e6d6f9540fef48d833d8af.jpg: 416x416 8 cs, 6.6ms\nimage 27/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_09_png.rf.c47d7daf1c117acb79a82f0b16edbfae.jpg: 416x416 12 cs, 6.5ms\nimage 28/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_11_png.rf.c764badc553ebd7c9893e0be843e134f.jpg: 416x416 9 cs, 6.5ms\nimage 29/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_16_png.rf.a64a46de9434e21c7455d71d209369dd.jpg: 416x416 11 cs, 6.5ms\nimage 30/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_18_png.rf.b65f0023274748b9a404e347de55d35d.jpg: 416x416 10 cs, 6.4ms\nimage 31/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_25_png.rf.5ea0dd56e58ae897e3fbb1565808c72c.jpg: 416x416 6 cs, 6.5ms\nimage 32/32 /kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_48_png.rf.a9d3e2d8f0ae863c1589fcd8f21c9c3c.jpg: 416x416 6 cs, 6.3ms\nSpeed: 1.1ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 416)\nResults saved to \u001b[1mruns/detect/predict\u001b[0m\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 72, 122, 122],\n         [ 54, 102, 104],\n         [ 43,  81,  86],\n         ...,\n         [ 57,  65,  82],\n         [ 57,  65,  82],\n         [ 53,  61,  78]],\n \n        [[ 55,  99, 100],\n         [ 54,  95,  98],\n         [ 41,  75,  81],\n         ...,\n         [ 58,  66,  83],\n         [ 50,  58,  75],\n         [ 42,  50,  67]],\n \n        [[ 46,  79,  82],\n         [ 50,  80,  85],\n         [ 30,  56,  63],\n         ...,\n         [ 48,  55,  74],\n         [ 36,  43,  62],\n         [ 26,  33,  52]],\n \n        ...,\n \n        [[ 10,  70,  62],\n         [ 11,  72,  62],\n         [ 20,  82,  70],\n         ...,\n         [ 48,  57,  77],\n         [ 50,  57,  77],\n         [ 44,  51,  71]],\n \n        [[ 45, 119, 101],\n         [ 41, 116,  95],\n         [ 35, 110,  88],\n         ...,\n         [ 33,  42,  62],\n         [ 47,  54,  74],\n         [ 48,  55,  75]],\n \n        [[ 72, 154, 131],\n         [ 62, 145, 120],\n         [ 42, 126,  98],\n         ...,\n         [ 34,  43,  63],\n         [ 41,  48,  68],\n         [ 38,  45,  65]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_17_png.rf.2cbfe9d703e1046a6bd252360814d983.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3616085052490234, 'inference': 6.691932678222656, 'postprocess': 1.91497802734375},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 44,  58,  56],\n         [ 44,  58,  56],\n         [ 68,  77,  80],\n         ...,\n         [ 82,  89, 109],\n         [ 73,  79,  98],\n         [ 68,  74,  93]],\n \n        [[ 39,  53,  52],\n         [ 31,  45,  44],\n         [ 45,  57,  59],\n         ...,\n         [ 85,  92, 112],\n         [ 76,  82, 101],\n         [ 77,  83, 102]],\n \n        [[ 23,  38,  41],\n         [ 21,  36,  39],\n         [ 35,  49,  55],\n         ...,\n         [ 90,  97, 117],\n         [ 81,  87, 106],\n         [ 89,  95, 114]],\n \n        ...,\n \n        [[ 75, 132, 124],\n         [ 93, 152, 144],\n         [129, 190, 180],\n         ...,\n         [ 89,  96, 121],\n         [ 66,  73, 100],\n         [ 59,  66,  93]],\n \n        [[ 60, 112, 105],\n         [ 59, 113, 106],\n         [ 69, 126, 117],\n         ...,\n         [ 99, 108, 135],\n         [ 81,  89, 118],\n         [ 80,  88, 117]],\n \n        [[ 29,  80,  73],\n         [ 36,  89,  80],\n         [ 34,  89,  80],\n         ...,\n         [107, 116, 143],\n         [ 89,  97, 126],\n         [ 75,  83, 112]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_20_png.rf.b42eb18de55a877b0e327be4c2596bf9.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.466512680053711, 'inference': 8.887052536010742, 'postprocess': 1.3806819915771484},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 39,  42,  46],\n         [ 41,  44,  48],\n         [ 43,  46,  50],\n         ...,\n         [ 94, 142, 118],\n         [104, 150, 128],\n         [110, 153, 132]],\n \n        [[ 40,  43,  48],\n         [ 40,  43,  47],\n         [ 36,  39,  44],\n         ...,\n         [ 79, 121, 103],\n         [ 83, 122, 106],\n         [ 79, 116, 100]],\n \n        [[ 30,  33,  41],\n         [ 32,  36,  41],\n         [ 29,  32,  40],\n         ...,\n         [ 58,  89,  82],\n         [ 61,  89,  83],\n         [ 56,  84,  78]],\n \n        ...,\n \n        [[ 83,  83,  99],\n         [ 66,  67,  81],\n         [ 49,  50,  64],\n         ...,\n         [ 64,  72,  89],\n         [ 63,  70,  87],\n         [ 60,  64,  82]],\n \n        [[104, 104, 122],\n         [ 89,  89, 105],\n         [ 74,  74,  90],\n         ...,\n         [ 74,  91, 104],\n         [ 69,  85,  98],\n         [ 64,  80,  93]],\n \n        [[ 89,  89, 107],\n         [ 83,  83, 101],\n         [ 79,  79,  95],\n         ...,\n         [ 62,  84,  96],\n         [ 73,  92, 105],\n         [ 88, 107, 120]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_25_png.rf.c782ccc3452d9c73651d90080c7fa248.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.8106231689453125, 'inference': 6.710529327392578, 'postprocess': 1.2676715850830078},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 98, 129, 130],\n         [ 99, 129, 130],\n         [126, 146, 151],\n         ...,\n         [ 69, 158, 132],\n         [ 55, 149, 124],\n         [ 55, 151, 127]],\n \n        [[ 84, 118, 118],\n         [ 87, 118, 119],\n         [ 99, 124, 128],\n         ...,\n         [ 38, 122,  94],\n         [ 27, 114,  88],\n         [ 35, 124,  98]],\n \n        [[ 56,  96,  95],\n         [ 58,  96,  96],\n         [ 57,  91,  91],\n         ...,\n         [  2,  72,  41],\n         [  0,  66,  36],\n         [  9,  83,  53]],\n \n        ...,\n \n        [[ 63,  70,  87],\n         [ 63,  70,  87],\n         [ 66,  73,  90],\n         ...,\n         [ 61,  86,  96],\n         [ 70,  92, 103],\n         [ 75,  97, 108]],\n \n        [[ 73,  80,  97],\n         [ 71,  78,  95],\n         [ 70,  77,  94],\n         ...,\n         [ 65,  89, 101],\n         [ 71,  93, 105],\n         [ 66,  88, 100]],\n \n        [[ 72,  79,  96],\n         [ 73,  80,  97],\n         [ 73,  80,  97],\n         ...,\n         [ 73,  96, 111],\n         [ 77,  98, 113],\n         [ 66,  87, 102]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_28_png.rf.9749fcf39aea7932df9a36f8645b7381.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7531642913818359, 'inference': 6.363153457641602, 'postprocess': 1.2357234954833984},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 72,  89, 102],\n         [ 66,  83,  96],\n         [ 61,  74,  90],\n         ...,\n         [ 21,  38,  47],\n         [ 37,  53,  60],\n         [ 54,  70,  77]],\n \n        [[ 68,  81,  95],\n         [ 65,  78,  92],\n         [ 60,  71,  85],\n         ...,\n         [ 39,  55,  61],\n         [ 41,  57,  63],\n         [ 49,  64,  67]],\n \n        [[ 76,  82,  95],\n         [ 67,  73,  86],\n         [ 51,  57,  70],\n         ...,\n         [ 71,  87,  86],\n         [ 64,  78,  77],\n         [ 57,  71,  69]],\n \n        ...,\n \n        [[119, 154, 188],\n         [113, 150, 184],\n         [109, 146, 180],\n         ...,\n         [ 91, 138, 169],\n         [ 94, 141, 172],\n         [ 87, 134, 165]],\n \n        [[129, 161, 197],\n         [115, 147, 183],\n         [ 96, 132, 168],\n         ...,\n         [ 92, 138, 169],\n         [ 95, 141, 172],\n         [ 90, 136, 167]],\n \n        [[127, 156, 193],\n         [110, 142, 178],\n         [ 93, 127, 163],\n         ...,\n         [ 92, 138, 169],\n         [ 92, 136, 167],\n         [103, 147, 178]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_33_png.rf.8206f2b80887a56d425abdfcaf22ab49.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2936592102050781, 'inference': 6.475210189819336, 'postprocess': 1.2366771697998047},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 45,  41,  52],\n         [ 47,  46,  56],\n         [ 52,  50,  62],\n         ...,\n         [ 75,  84, 104],\n         [ 85,  92, 112],\n         [ 92,  99, 119]],\n \n        [[ 36,  32,  43],\n         [ 41,  40,  50],\n         [ 48,  48,  60],\n         ...,\n         [ 59,  68,  88],\n         [ 71,  78,  98],\n         [ 80,  87, 107]],\n \n        [[ 33,  32,  42],\n         [ 36,  35,  45],\n         [ 39,  39,  51],\n         ...,\n         [ 45,  52,  72],\n         [ 53,  60,  80],\n         [ 61,  68,  88]],\n \n        ...,\n \n        [[ 15,  18,  26],\n         [ 44,  47,  55],\n         [ 83,  85,  93],\n         ...,\n         [ 70,  82, 100],\n         [ 57,  69,  87],\n         [ 50,  62,  80]],\n \n        [[ 17,  21,  26],\n         [ 42,  46,  51],\n         [ 80,  83,  88],\n         ...,\n         [ 67,  79,  97],\n         [ 49,  61,  79],\n         [ 35,  47,  65]],\n \n        [[ 15,  20,  23],\n         [ 32,  37,  40],\n         [ 68,  71,  76],\n         ...,\n         [ 64,  76,  94],\n         [ 45,  57,  75],\n         [ 29,  41,  59]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_35_png.rf.d5391a0424a348be3bef0010be20801f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7011890411376953, 'inference': 6.379604339599609, 'postprocess': 1.2273788452148438},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 61,  66,  87],\n         [ 61,  66,  87],\n         [ 57,  64,  84],\n         ...,\n         [ 71,  86, 112],\n         [ 70,  81, 108],\n         [ 63,  74, 101]],\n \n        [[ 59,  64,  85],\n         [ 61,  66,  87],\n         [ 63,  68,  89],\n         ...,\n         [ 66,  81, 107],\n         [ 71,  82, 109],\n         [ 72,  83, 110]],\n \n        [[ 58,  62,  81],\n         [ 60,  66,  85],\n         [ 67,  73,  92],\n         ...,\n         [ 74,  89, 115],\n         [ 81,  92, 119],\n         [ 85,  96, 123]],\n \n        ...,\n \n        [[ 61,  68,  83],\n         [ 62,  69,  84],\n         [ 60,  67,  82],\n         ...,\n         [101, 117, 140],\n         [113, 127, 149],\n         [120, 134, 156]],\n \n        [[ 58,  65,  80],\n         [ 63,  70,  85],\n         [ 65,  72,  87],\n         ...,\n         [ 93, 109, 132],\n         [108, 122, 144],\n         [119, 133, 155]],\n \n        [[ 54,  61,  76],\n         [ 63,  70,  85],\n         [ 69,  76,  91],\n         ...,\n         [ 84, 100, 123],\n         [ 93, 107, 129],\n         [101, 115, 137]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/testing_48_png.rf.fc2f106cbecb2aca31605850989bfd78.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2359619140625, 'inference': 6.382942199707031, 'postprocess': 1.2233257293701172},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[106, 125, 146],\n         [ 67,  88, 109],\n         [ 35,  57,  82],\n         ...,\n         [ 79,  85,  96],\n         [ 85,  90,  99],\n         [ 78,  83,  92]],\n \n        [[ 91, 109, 132],\n         [ 66,  87, 109],\n         [ 46,  68,  93],\n         ...,\n         [ 71,  80,  90],\n         [ 77,  84,  93],\n         [ 72,  79,  88]],\n \n        [[ 84, 104, 129],\n         [ 77,  99, 124],\n         [ 73,  96, 122],\n         ...,\n         [ 50,  62,  72],\n         [ 59,  70,  78],\n         [ 58,  69,  77]],\n \n        ...,\n \n        [[ 92, 112, 147],\n         [ 99, 120, 152],\n         [119, 140, 172],\n         ...,\n         [ 37,  49,  53],\n         [ 43,  55,  59],\n         [ 49,  62,  64]],\n \n        [[ 96, 118, 154],\n         [105, 127, 162],\n         [130, 150, 185],\n         ...,\n         [ 37,  49,  53],\n         [ 42,  54,  58],\n         [ 43,  58,  61]],\n \n        [[112, 136, 172],\n         [108, 132, 168],\n         [123, 143, 178],\n         ...,\n         [ 42,  54,  60],\n         [ 39,  54,  57],\n         [ 37,  52,  55]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_105_png.rf.654dfc59c220ab8e879d185ae947fd0e.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7236003875732422, 'inference': 6.4373016357421875, 'postprocess': 1.224517822265625},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 30,  65,  75],\n         [ 68, 102, 115],\n         [ 89, 122, 142],\n         ...,\n         [122, 147, 179],\n         [109, 134, 166],\n         [103, 128, 160]],\n \n        [[ 49,  85,  93],\n         [ 68, 103, 116],\n         [ 81, 114, 133],\n         ...,\n         [108, 133, 165],\n         [ 88, 113, 145],\n         [ 84, 109, 141]],\n \n        [[ 58,  96, 101],\n         [ 75, 112, 120],\n         [ 94, 130, 146],\n         ...,\n         [108, 133, 165],\n         [ 87, 112, 144],\n         [ 85, 110, 142]],\n \n        ...,\n \n        [[ 77, 144, 139],\n         [ 81, 147, 142],\n         [ 90, 151, 147],\n         ...,\n         [ 91, 113, 148],\n         [109, 131, 166],\n         [ 92, 114, 149]],\n \n        [[ 62, 134, 122],\n         [ 76, 145, 134],\n         [100, 166, 155],\n         ...,\n         [ 76,  98, 133],\n         [111, 133, 169],\n         [114, 136, 172]],\n \n        [[ 51, 124, 108],\n         [ 71, 142, 126],\n         [106, 173, 158],\n         ...,\n         [ 61,  83, 118],\n         [ 99, 121, 157],\n         [135, 157, 193]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_121_png.rf.f57d5de666f070bc561ff20c721c8e8f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2378692626953125, 'inference': 6.333112716674805, 'postprocess': 1.2252330780029297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 67,  88, 115],\n         [ 75,  98, 124],\n         [ 95, 118, 144],\n         ...,\n         [111, 136, 168],\n         [105, 130, 162],\n         [ 91, 116, 148]],\n \n        [[ 68,  89, 116],\n         [ 78, 101, 127],\n         [ 98, 123, 149],\n         ...,\n         [119, 144, 176],\n         [113, 138, 170],\n         [104, 129, 161]],\n \n        [[ 81, 103, 131],\n         [ 91, 113, 141],\n         [105, 129, 157],\n         ...,\n         [122, 147, 179],\n         [114, 139, 171],\n         [109, 134, 166]],\n \n        ...,\n \n        [[ 80, 102, 137],\n         [ 83, 105, 140],\n         [ 85, 108, 140],\n         ...,\n         [100, 125, 159],\n         [101, 126, 160],\n         [104, 129, 163]],\n \n        [[ 71,  93, 128],\n         [ 80, 102, 137],\n         [ 79, 102, 134],\n         ...,\n         [101, 126, 160],\n         [102, 127, 161],\n         [105, 130, 164]],\n \n        [[ 76,  98, 133],\n         [ 90, 112, 147],\n         [ 87, 110, 142],\n         ...,\n         [ 99, 124, 158],\n         [ 98, 123, 157],\n         [ 99, 124, 158]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_122_png.rf.e0a857770d48399eb4e8ed75f0469b9e.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7317066192626953, 'inference': 6.426334381103516, 'postprocess': 1.2581348419189453},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[109, 132, 147],\n         [ 89, 113, 125],\n         [ 72,  95, 103],\n         ...,\n         [ 84,  89, 110],\n         [ 61,  64,  85],\n         [ 47,  50,  71]],\n \n        [[109, 141, 152],\n         [104, 136, 142],\n         [ 89, 119, 124],\n         ...,\n         [ 64,  71,  91],\n         [ 57,  62,  83],\n         [ 58,  63,  84]],\n \n        [[114, 165, 167],\n         [117, 167, 165],\n         [105, 150, 147],\n         ...,\n         [ 47,  55,  78],\n         [ 44,  50,  73],\n         [ 44,  50,  73]],\n \n        ...,\n \n        [[ 58, 108, 114],\n         [ 61, 112, 115],\n         [ 65, 116, 119],\n         ...,\n         [ 76, 101, 135],\n         [105, 129, 165],\n         [117, 143, 179]],\n \n        [[ 68, 113, 147],\n         [ 73, 118, 151],\n         [ 74, 118, 147],\n         ...,\n         [ 94, 119, 153],\n         [110, 134, 170],\n         [119, 143, 179]],\n \n        [[ 71, 112, 161],\n         [ 81, 123, 170],\n         [ 84, 124, 166],\n         ...,\n         [118, 141, 173],\n         [118, 142, 178],\n         [124, 148, 184]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_124_png.rf.880cf55a745bd87522d9ac1cf09334be.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2211799621582031, 'inference': 6.511211395263672, 'postprocess': 1.2645721435546875},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 91, 106, 138],\n         [ 83,  98, 130],\n         [ 92, 110, 141],\n         ...,\n         [110, 142, 183],\n         [115, 147, 188],\n         [117, 149, 190]],\n \n        [[105, 120, 152],\n         [ 92, 110, 141],\n         [ 86, 104, 135],\n         ...,\n         [ 82, 115, 154],\n         [100, 130, 171],\n         [111, 144, 183]],\n \n        [[117, 134, 167],\n         [102, 121, 154],\n         [ 82, 101, 134],\n         ...,\n         [ 57,  89, 125],\n         [ 85, 114, 153],\n         [106, 138, 174]],\n \n        ...,\n \n        [[ 67,  84, 117],\n         [ 78,  97, 130],\n         [ 90, 109, 142],\n         ...,\n         [ 81, 107, 137],\n         [ 81, 107, 137],\n         [ 83, 109, 139]],\n \n        [[ 72,  89, 122],\n         [ 75,  94, 127],\n         [ 78,  97, 130],\n         ...,\n         [ 70,  97, 124],\n         [ 76, 103, 130],\n         [ 82, 109, 136]],\n \n        [[ 86, 103, 136],\n         [ 74,  93, 126],\n         [ 61,  80, 113],\n         ...,\n         [ 65,  92, 119],\n         [ 76, 103, 130],\n         [ 86, 113, 140]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_12_png.rf.896d9610e8b3109bb4d498202c89c2de.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2507438659667969, 'inference': 6.315946578979492, 'postprocess': 1.2869834899902344},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[109, 155, 143],\n         [110, 156, 144],\n         [105, 152, 143],\n         ...,\n         [ 79, 100, 128],\n         [ 75,  96, 124],\n         [ 66,  87, 115]],\n \n        [[112, 157, 148],\n         [ 91, 136, 127],\n         [ 72, 119, 110],\n         ...,\n         [ 88, 109, 137],\n         [ 79, 100, 128],\n         [ 64,  85, 113]],\n \n        [[125, 169, 162],\n         [ 90, 134, 127],\n         [ 63, 110, 102],\n         ...,\n         [ 95, 116, 144],\n         [ 86, 107, 135],\n         [ 76,  97, 125]],\n \n        ...,\n \n        [[ 90, 125, 135],\n         [ 98, 133, 143],\n         [100, 134, 147],\n         ...,\n         [ 57,  66,  86],\n         [ 50,  56,  79],\n         [ 40,  49,  69]],\n \n        [[121, 157, 165],\n         [121, 157, 165],\n         [117, 149, 160],\n         ...,\n         [ 44,  51,  70],\n         [ 52,  57,  78],\n         [ 65,  72,  91]],\n \n        [[127, 163, 169],\n         [128, 164, 170],\n         [126, 159, 168],\n         ...,\n         [ 36,  42,  61],\n         [ 58,  64,  83],\n         [ 91,  97, 116]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_134_png.rf.41ef2a70982eb3781162c1f9428e7a34.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2407302856445312, 'inference': 6.552219390869141, 'postprocess': 1.2218952178955078},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[111, 135, 159],\n         [ 86, 110, 134],\n         [ 64,  88, 112],\n         ...,\n         [ 99, 141, 116],\n         [ 87, 126, 100],\n         [ 72, 109,  83]],\n \n        [[ 68,  92, 116],\n         [ 65,  89, 113],\n         [ 65,  89, 113],\n         ...,\n         [ 86, 131, 105],\n         [ 69, 110,  83],\n         [ 48,  87,  61]],\n \n        [[ 35,  59,  83],\n         [ 42,  66,  90],\n         [ 54,  78, 102],\n         ...,\n         [ 62, 111,  83],\n         [ 33,  81,  53],\n         [  9,  54,  27]],\n \n        ...,\n \n        [[143, 184, 179],\n         [136, 177, 172],\n         [130, 171, 166],\n         ...,\n         [ 59,  82,  84],\n         [ 62,  89,  93],\n         [ 74, 103, 107]],\n \n        [[ 95, 136, 138],\n         [ 86, 127, 129],\n         [ 78, 119, 121],\n         ...,\n         [ 59,  75,  82],\n         [ 76,  95, 103],\n         [ 93, 114, 122]],\n \n        [[ 59, 102, 105],\n         [ 48,  91,  94],\n         [ 42,  82,  87],\n         ...,\n         [ 67,  79,  89],\n         [ 69,  85,  97],\n         [ 70,  88,  99]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_160_png.rf.3e1a825bd9b8af6aedb164f527c1b412.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7176399230957031, 'inference': 6.788492202758789, 'postprocess': 1.3086795806884766},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[104, 128, 158],\n         [118, 142, 172],\n         [118, 142, 172],\n         ...,\n         [ 96, 115, 148],\n         [ 95, 114, 147],\n         [ 98, 117, 150]],\n \n        [[120, 144, 174],\n         [119, 143, 173],\n         [106, 130, 160],\n         ...,\n         [ 90, 108, 139],\n         [ 89, 107, 138],\n         [ 91, 109, 140]],\n \n        [[120, 144, 174],\n         [108, 132, 162],\n         [ 91, 115, 145],\n         ...,\n         [ 81,  97, 126],\n         [ 87, 103, 132],\n         [ 89, 105, 134]],\n \n        ...,\n \n        [[113, 133, 164],\n         [107, 127, 158],\n         [101, 119, 150],\n         ...,\n         [ 93, 107, 129],\n         [ 91, 105, 127],\n         [ 81,  98, 119]],\n \n        [[113, 132, 165],\n         [105, 124, 157],\n         [ 96, 113, 146],\n         ...,\n         [ 92, 105, 127],\n         [ 91, 104, 126],\n         [ 81,  95, 117]],\n \n        [[102, 121, 154],\n         [ 92, 111, 144],\n         [ 84, 101, 134],\n         ...,\n         [ 81,  92, 114],\n         [ 78,  91, 113],\n         [ 71,  84, 106]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_166_png.rf.9cdd602f0662e345ab143cb4ebc58edb.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7441043853759766, 'inference': 6.674051284790039, 'postprocess': 1.2545585632324219},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[131, 153, 181],\n         [126, 148, 176],\n         [125, 149, 179],\n         ...,\n         [ 86, 113, 140],\n         [ 89, 115, 145],\n         [ 89, 115, 145]],\n \n        [[130, 152, 180],\n         [124, 146, 174],\n         [122, 146, 176],\n         ...,\n         [ 82, 109, 136],\n         [ 86, 112, 142],\n         [ 87, 113, 143]],\n \n        [[119, 141, 169],\n         [114, 136, 164],\n         [112, 136, 166],\n         ...,\n         [ 82, 106, 134],\n         [ 87, 111, 139],\n         [ 89, 113, 141]],\n \n        ...,\n \n        [[ 73, 175, 147],\n         [ 63, 163, 135],\n         [ 57, 151, 126],\n         ...,\n         [111, 162, 158],\n         [ 94, 148, 148],\n         [ 74, 130, 131]],\n \n        [[ 77, 194, 161],\n         [ 67, 181, 151],\n         [ 56, 166, 138],\n         ...,\n         [110, 165, 156],\n         [103, 161, 156],\n         [ 93, 153, 147]],\n \n        [[ 78, 204, 169],\n         [ 71, 194, 160],\n         [ 64, 180, 151],\n         ...,\n         [101, 159, 148],\n         [108, 168, 158],\n         [111, 171, 163]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_188_png.rf.9745ecfcfe31aa4fb165e008c2898deb.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7367134094238281, 'inference': 6.679296493530273, 'postprocess': 1.3997554779052734},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[117, 140, 178],\n         [110, 133, 171],\n         [ 99, 123, 159],\n         ...,\n         [ 98, 114, 143],\n         [106, 118, 152],\n         [108, 120, 154]],\n \n        [[110, 136, 172],\n         [112, 139, 173],\n         [108, 135, 169],\n         ...,\n         [ 78,  97, 124],\n         [101, 119, 148],\n         [117, 135, 164]],\n \n        [[107, 137, 166],\n         [111, 141, 168],\n         [105, 135, 162],\n         ...,\n         [ 63,  90, 110],\n         [ 94, 121, 142],\n         [116, 143, 164]],\n \n        ...,\n \n        [[120, 135, 167],\n         [104, 119, 151],\n         [ 73,  91, 122],\n         ...,\n         [ 90, 122, 151],\n         [105, 140, 173],\n         [113, 151, 183]],\n \n        [[123, 138, 170],\n         [109, 124, 156],\n         [ 74,  92, 123],\n         ...,\n         [106, 140, 169],\n         [ 98, 136, 168],\n         [106, 145, 177]],\n \n        [[145, 160, 192],\n         [124, 139, 171],\n         [ 80,  98, 129],\n         ...,\n         [126, 163, 191],\n         [103, 141, 173],\n         [ 99, 138, 170]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_18_png.rf.945f918b7846052452e031aec2028822.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.751495361328125, 'inference': 9.312629699707031, 'postprocess': 1.772165298461914},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 86, 147, 113],\n         [ 81, 145, 110],\n         [ 84, 149, 117],\n         ...,\n         [108, 135, 149],\n         [ 82, 111, 126],\n         [ 91, 120, 135]],\n \n        [[ 87, 145, 117],\n         [ 90, 149, 121],\n         [105, 166, 138],\n         ...,\n         [111, 137, 153],\n         [ 95, 123, 140],\n         [ 85, 113, 130]],\n \n        [[138, 187, 173],\n         [134, 185, 171],\n         [141, 194, 180],\n         ...,\n         [ 96, 122, 139],\n         [105, 132, 152],\n         [102, 129, 149]],\n \n        ...,\n \n        [[ 58,  63,  88],\n         [ 62,  69,  94],\n         [ 71,  82, 109],\n         ...,\n         [ 79,  96, 122],\n         [ 69,  86, 112],\n         [ 56,  73,  99]],\n \n        [[ 65,  69,  93],\n         [ 53,  60,  85],\n         [ 57,  68,  95],\n         ...,\n         [ 75,  92, 118],\n         [ 58,  75, 101],\n         [ 38,  55,  81]],\n \n        [[ 73,  77, 101],\n         [ 48,  53,  78],\n         [ 42,  53,  80],\n         ...,\n         [ 75,  92, 118],\n         [ 56,  73,  99],\n         [ 32,  49,  75]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_192_png.rf.aa94ed3b709c89860f1a52d0c02b975f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.253366470336914, 'inference': 6.483554840087891, 'postprocess': 1.2547969818115234},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 97, 113, 156],\n         [101, 117, 159],\n         [ 89, 105, 147],\n         ...,\n         [ 62,  80, 109],\n         [ 71,  87, 116],\n         [ 69,  85, 114]],\n \n        [[ 99, 115, 158],\n         [ 88, 104, 146],\n         [ 63,  79, 121],\n         ...,\n         [ 64,  82, 111],\n         [ 71,  87, 116],\n         [ 73,  89, 118]],\n \n        [[100, 116, 159],\n         [ 87, 103, 145],\n         [ 62,  80, 121],\n         ...,\n         [ 71,  89, 118],\n         [ 78,  94, 123],\n         [ 84, 100, 129]],\n \n        ...,\n \n        [[ 94, 112, 141],\n         [ 86, 104, 133],\n         [ 80, 101, 129],\n         ...,\n         [ 82, 104, 132],\n         [ 91, 112, 140],\n         [ 96, 117, 145]],\n \n        [[102, 118, 147],\n         [ 95, 113, 142],\n         [ 78,  99, 127],\n         ...,\n         [ 89, 111, 139],\n         [ 99, 120, 148],\n         [104, 125, 153]],\n \n        [[108, 124, 153],\n         [106, 124, 153],\n         [ 81,  99, 128],\n         ...,\n         [ 90, 112, 140],\n         [ 98, 119, 147],\n         [102, 123, 151]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_196_png.rf.83dca24251b5280cc1084e7fb722e8c9.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.268625259399414, 'inference': 6.650447845458984, 'postprocess': 1.2230873107910156},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 59, 106,  80],\n         [ 56, 102,  79],\n         [ 49,  95,  73],\n         ...,\n         [ 76,  98, 126],\n         [ 80, 104, 132],\n         [ 85, 109, 137]],\n \n        [[ 69, 115,  92],\n         [ 75, 121,  99],\n         [ 81, 126, 107],\n         ...,\n         [ 72,  94, 122],\n         [ 79, 103, 131],\n         [ 86, 110, 138]],\n \n        [[ 78, 120, 102],\n         [100, 142, 125],\n         [129, 170, 155],\n         ...,\n         [ 74,  96, 124],\n         [ 84, 108, 136],\n         [ 91, 115, 143]],\n \n        ...,\n \n        [[ 46,  50,  68],\n         [ 45,  49,  67],\n         [ 65,  71,  90],\n         ...,\n         [ 91, 111, 142],\n         [102, 123, 155],\n         [102, 123, 155]],\n \n        [[ 45,  49,  67],\n         [ 52,  56,  74],\n         [ 86,  92, 111],\n         ...,\n         [102, 123, 154],\n         [103, 124, 156],\n         [ 91, 112, 144]],\n \n        [[ 48,  52,  70],\n         [ 63,  67,  85],\n         [104, 110, 129],\n         ...,\n         [119, 140, 171],\n         [112, 133, 165],\n         [ 90, 111, 143]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_44_png.rf.a23efb05fa7977f94870fa3886fec577.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7481575012207031, 'inference': 6.535053253173828, 'postprocess': 1.2404918670654297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 95, 137, 136],\n         [ 85, 132, 130],\n         [ 69, 120, 116],\n         ...,\n         [ 77,  94, 113],\n         [ 82,  99, 118],\n         [ 87, 105, 122]],\n \n        [[128, 165, 163],\n         [121, 160, 158],\n         [104, 150, 144],\n         ...,\n         [ 61,  78,  97],\n         [ 68,  85, 104],\n         [ 75,  93, 110]],\n \n        [[171, 196, 192],\n         [163, 190, 186],\n         [149, 182, 175],\n         ...,\n         [ 37,  54,  73],\n         [ 39,  56,  75],\n         [ 42,  60,  77]],\n \n        ...,\n \n        [[ 34,  56,  68],\n         [ 53,  81,  88],\n         [ 81, 114, 117],\n         ...,\n         [ 79, 100, 122],\n         [ 87, 108, 130],\n         [ 91, 112, 134]],\n \n        [[ 47,  72,  82],\n         [ 61,  90,  97],\n         [ 77, 112, 115],\n         ...,\n         [ 67,  88, 110],\n         [ 76,  97, 119],\n         [ 90, 111, 133]],\n \n        [[ 67,  94, 104],\n         [ 67,  96, 103],\n         [ 65, 103, 103],\n         ...,\n         [ 61,  82, 104],\n         [ 66,  87, 109],\n         [ 83, 104, 126]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_53_png.rf.9a18f731eaaf83ab30927ca37379e142.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 0.7636547088623047, 'inference': 6.515979766845703, 'postprocess': 1.2459754943847656},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 91, 104, 136],\n         [ 85,  99, 128],\n         [ 82,  93, 123],\n         ...,\n         [ 69,  82, 108],\n         [ 87, 102, 128],\n         [102, 120, 143]],\n \n        [[ 84,  97, 129],\n         [ 78,  92, 121],\n         [ 75,  87, 115],\n         ...,\n         [ 64,  78, 106],\n         [ 78,  95, 121],\n         [ 93, 114, 136]],\n \n        [[ 82,  98, 127],\n         [ 77,  91, 120],\n         [ 73,  85, 113],\n         ...,\n         [ 58,  75, 102],\n         [ 59,  80, 107],\n         [ 67,  91, 115]],\n \n        ...,\n \n        [[ 84, 105, 126],\n         [ 84, 105, 126],\n         [ 72,  93, 114],\n         ...,\n         [101, 134, 143],\n         [ 76, 109, 118],\n         [ 57,  93, 101]],\n \n        [[ 80, 100, 117],\n         [ 75,  95, 112],\n         [ 68,  88, 106],\n         ...,\n         [103, 137, 150],\n         [ 76, 110, 123],\n         [ 56,  91, 104]],\n \n        [[ 88, 109, 124],\n         [ 79, 100, 115],\n         [ 72,  90, 107],\n         ...,\n         [115, 150, 164],\n         [ 81, 116, 130],\n         [ 58,  93, 107]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_75_png.rf.23155c3496205e4094459310f857b293.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3175010681152344, 'inference': 6.314754486083984, 'postprocess': 1.2252330780029297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[127, 139, 167],\n         [138, 150, 178],\n         [132, 147, 173],\n         ...,\n         [ 40,  62,  90],\n         [ 37,  57,  92],\n         [ 58,  80, 116]],\n \n        [[ 90, 101, 131],\n         [ 94, 108, 136],\n         [ 93, 107, 135],\n         ...,\n         [ 72,  92, 117],\n         [ 56,  76, 107],\n         [ 43,  64,  96]],\n \n        [[ 44,  57,  89],\n         [ 42,  58,  87],\n         [ 43,  59,  88],\n         ...,\n         [ 90, 109, 124],\n         [ 66,  85, 106],\n         [ 35,  53,  76]],\n \n        ...,\n \n        [[ 99, 124, 156],\n         [ 96, 121, 153],\n         [ 90, 115, 147],\n         ...,\n         [ 66,  82,  99],\n         [ 47,  60,  76],\n         [ 40,  53,  69]],\n \n        [[ 93, 118, 150],\n         [ 88, 113, 145],\n         [ 79, 106, 140],\n         ...,\n         [ 79,  97, 114],\n         [ 63,  78,  94],\n         [ 60,  75,  91]],\n \n        [[ 87, 112, 144],\n         [ 85, 110, 142],\n         [ 78, 105, 139],\n         ...,\n         [ 93, 111, 128],\n         [ 84, 100, 116],\n         [ 84,  99, 115]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_78_png.rf.3ae8d098be942fa2848c56989bdeda92.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2421607971191406, 'inference': 6.565093994140625, 'postprocess': 1.2226104736328125},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 67,  81, 104],\n         [ 60,  74,  97],\n         [ 49,  64,  90],\n         ...,\n         [ 93, 121, 155],\n         [ 74, 105, 138],\n         [ 67,  98, 131]],\n \n        [[ 63,  77, 100],\n         [ 54,  68,  91],\n         [ 38,  53,  79],\n         ...,\n         [ 93, 121, 155],\n         [ 76, 107, 140],\n         [ 70, 101, 134]],\n \n        [[ 67,  81, 104],\n         [ 58,  72,  95],\n         [ 42,  57,  83],\n         ...,\n         [ 90, 118, 152],\n         [ 76, 107, 140],\n         [ 73, 104, 137]],\n \n        ...,\n \n        [[ 87, 115, 145],\n         [ 76, 104, 134],\n         [ 77, 105, 135],\n         ...,\n         [111, 136, 168],\n         [ 88, 115, 142],\n         [ 73, 101, 125]],\n \n        [[ 83, 110, 137],\n         [ 80, 107, 134],\n         [ 85, 112, 139],\n         ...,\n         [108, 130, 165],\n         [ 84, 108, 138],\n         [ 71,  96, 122]],\n \n        [[ 81, 108, 135],\n         [ 83, 110, 137],\n         [ 92, 119, 146],\n         ...,\n         [100, 122, 158],\n         [ 81, 102, 133],\n         [ 72,  94, 122]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_82_png.rf.73e51476d5cdf1f15e0006114582c3a2.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3060569763183594, 'inference': 6.439685821533203, 'postprocess': 1.2116432189941406},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[108, 138, 167],\n         [109, 139, 168],\n         [109, 139, 168],\n         ...,\n         [109, 129, 160],\n         [119, 139, 170],\n         [120, 140, 171]],\n \n        [[100, 130, 159],\n         [102, 132, 161],\n         [105, 135, 164],\n         ...,\n         [102, 122, 153],\n         [114, 134, 165],\n         [120, 140, 171]],\n \n        [[ 94, 123, 154],\n         [ 98, 127, 158],\n         [103, 132, 163],\n         ...,\n         [ 90, 110, 141],\n         [ 94, 114, 145],\n         [ 97, 117, 148]],\n \n        ...,\n \n        [[ 73, 157, 129],\n         [ 79, 163, 135],\n         [ 86, 168, 145],\n         ...,\n         [ 12,  33,  41],\n         [ 21,  41,  52],\n         [ 23,  45,  56]],\n \n        [[ 84, 159, 138],\n         [ 88, 162, 144],\n         [ 90, 162, 149],\n         ...,\n         [ 44,  64,  75],\n         [ 52,  74,  86],\n         [ 45,  69,  81]],\n \n        [[105, 176, 160],\n         [105, 175, 162],\n         [100, 169, 158],\n         ...,\n         [ 67,  89, 100],\n         [ 83, 104, 119],\n         [ 67,  90, 105]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_89_png.rf.21b9d634f712eb1263ae6891e20da1a1.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2204647064208984, 'inference': 6.676673889160156, 'postprocess': 1.1906623840332031},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 67,  88, 116],\n         [ 76,  97, 125],\n         [ 97, 118, 149],\n         ...,\n         [ 83, 102, 137],\n         [102, 127, 161],\n         [117, 144, 178]],\n \n        [[ 71,  92, 120],\n         [ 68,  89, 117],\n         [ 76,  97, 128],\n         ...,\n         [ 90, 109, 144],\n         [105, 127, 162],\n         [115, 142, 176]],\n \n        [[ 76,  97, 125],\n         [ 63,  84, 112],\n         [ 61,  82, 113],\n         ...,\n         [ 85, 104, 137],\n         [102, 125, 157],\n         [116, 141, 173]],\n \n        ...,\n \n        [[ 47,  63,  92],\n         [ 59,  75, 104],\n         [ 71,  87, 116],\n         ...,\n         [100, 121, 148],\n         [110, 133, 159],\n         [100, 123, 149]],\n \n        [[ 59,  75, 104],\n         [ 69,  85, 114],\n         [ 80,  96, 125],\n         ...,\n         [ 78,  99, 127],\n         [ 81, 103, 131],\n         [ 78, 100, 128]],\n \n        [[ 86, 102, 131],\n         [ 90, 106, 135],\n         [ 92, 108, 137],\n         ...,\n         [ 60,  81, 109],\n         [ 67,  88, 119],\n         [ 80, 101, 132]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/training_95_png.rf.143171f3e7e6d6f9540fef48d833d8af.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2493133544921875, 'inference': 6.557703018188477, 'postprocess': 1.2412071228027344},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 68, 103,  99],\n         [ 55,  90,  86],\n         [ 54,  87,  83],\n         ...,\n         [ 49,  61,  79],\n         [ 65,  79,  97],\n         [ 69,  83, 101]],\n \n        [[ 74, 109, 105],\n         [ 67, 102,  98],\n         [ 72, 107, 103],\n         ...,\n         [ 64,  76,  94],\n         [ 67,  81,  99],\n         [ 65,  79,  97]],\n \n        [[ 89, 126, 122],\n         [ 88, 125, 121],\n         [ 99, 136, 132],\n         ...,\n         [ 78,  90, 108],\n         [ 65,  79,  97],\n         [ 58,  72,  90]],\n \n        ...,\n \n        [[ 52,  59,  79],\n         [ 54,  61,  81],\n         [ 67,  76,  96],\n         ...,\n         [ 97, 109, 133],\n         [ 99, 112, 138],\n         [102, 115, 141]],\n \n        [[ 55,  61,  84],\n         [ 49,  55,  78],\n         [ 51,  59,  82],\n         ...,\n         [ 87,  98, 125],\n         [ 93, 104, 131],\n         [ 98, 109, 136]],\n \n        [[ 57,  63,  86],\n         [ 55,  61,  84],\n         [ 41,  49,  72],\n         ...,\n         [ 86,  97, 124],\n         [ 88,  99, 126],\n         [ 91, 102, 129]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_09_png.rf.c47d7daf1c117acb79a82f0b16edbfae.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.25885009765625, 'inference': 6.527423858642578, 'postprocess': 1.2583732604980469},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[116, 122, 151],\n         [ 98, 104, 133],\n         [ 78,  86, 116],\n         ...,\n         [ 47, 115,  98],\n         [ 97, 159, 147],\n         [113, 173, 162]],\n \n        [[111, 117, 146],\n         [ 97, 105, 134],\n         [ 83,  91, 120],\n         ...,\n         [ 33, 100,  85],\n         [ 79, 141, 129],\n         [112, 172, 162]],\n \n        [[ 91, 100, 127],\n         [ 87,  96, 123],\n         [ 81,  90, 117],\n         ...,\n         [ 40, 103,  93],\n         [ 67, 127, 119],\n         [ 93, 152, 144]],\n \n        ...,\n \n        [[114, 133, 146],\n         [ 98, 118, 129],\n         [ 83, 106, 114],\n         ...,\n         [ 55,  91,  99],\n         [ 55,  87, 100],\n         [ 42,  74,  87]],\n \n        [[ 99, 130, 127],\n         [ 97, 130, 126],\n         [100, 136, 130],\n         ...,\n         [ 85, 103, 120],\n         [ 84,  98, 120],\n         [ 79,  93, 115]],\n \n        [[ 85, 122, 112],\n         [108, 148, 136],\n         [140, 180, 168],\n         ...,\n         [103, 114, 136],\n         [108, 113, 138],\n         [116, 120, 148]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_11_png.rf.c764badc553ebd7c9893e0be843e134f.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2786388397216797, 'inference': 6.464719772338867, 'postprocess': 1.2276172637939453},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 73,  94,  91],\n         [ 70,  91,  89],\n         [ 72,  87,  89],\n         ...,\n         [ 68,  82, 101],\n         [ 72,  87, 106],\n         [ 81,  96, 115]],\n \n        [[ 75,  97,  95],\n         [ 70,  91,  92],\n         [ 68,  85,  88],\n         ...,\n         [ 71,  85, 104],\n         [ 69,  84, 103],\n         [ 73,  88, 107]],\n \n        [[ 83, 111, 112],\n         [ 75, 100, 104],\n         [ 69,  88,  95],\n         ...,\n         [ 79,  93, 112],\n         [ 73,  88, 107],\n         [ 73,  88, 107]],\n \n        ...,\n \n        [[ 48,  85,  81],\n         [ 34,  74,  69],\n         [ 33,  74,  69],\n         ...,\n         [ 71,  82, 102],\n         [ 71,  85, 103],\n         [ 72,  86, 104]],\n \n        [[ 68, 110, 103],\n         [ 59, 103,  96],\n         [ 64, 111, 103],\n         ...,\n         [ 68,  79,  99],\n         [ 68,  82, 100],\n         [ 71,  85, 103]],\n \n        [[ 74, 118, 111],\n         [ 81, 128, 120],\n         [ 95, 142, 134],\n         ...,\n         [ 67,  78,  98],\n         [ 66,  80,  98],\n         [ 69,  83, 101]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_16_png.rf.a64a46de9434e21c7455d71d209369dd.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3473033905029297, 'inference': 6.5460205078125, 'postprocess': 1.2466907501220703},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 93, 107, 126],\n         [ 98, 112, 131],\n         [ 88, 101, 123],\n         ...,\n         [ 78,  89, 111],\n         [ 69,  80, 102],\n         [ 73,  84, 106]],\n \n        [[ 96, 110, 129],\n         [ 96, 110, 129],\n         [ 82,  95, 117],\n         ...,\n         [ 75,  86, 108],\n         [ 68,  79, 101],\n         [ 73,  84, 106]],\n \n        [[ 91, 104, 126],\n         [ 90, 103, 125],\n         [ 77,  90, 112],\n         ...,\n         [ 74,  85, 107],\n         [ 69,  80, 102],\n         [ 73,  84, 106]],\n \n        ...,\n \n        [[ 74,  88, 107],\n         [ 75,  89, 108],\n         [ 75,  89, 108],\n         ...,\n         [ 71,  85, 104],\n         [ 69,  83, 102],\n         [ 67,  81, 100]],\n \n        [[ 73,  87, 106],\n         [ 74,  88, 107],\n         [ 74,  88, 107],\n         ...,\n         [ 71,  85, 104],\n         [ 73,  87, 106],\n         [ 75,  89, 108]],\n \n        [[ 75,  89, 108],\n         [ 76,  90, 109],\n         [ 75,  89, 108],\n         ...,\n         [ 72,  86, 105],\n         [ 78,  92, 111],\n         [ 83,  97, 116]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_18_png.rf.b65f0023274748b9a404e347de55d35d.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3341903686523438, 'inference': 6.387948989868164, 'postprocess': 1.4235973358154297},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 79,  95, 112],\n         [ 78,  94, 111],\n         [ 80,  95, 111],\n         ...,\n         [ 74,  83,  96],\n         [ 73,  81,  94],\n         [ 73,  81,  94]],\n \n        [[ 64,  80,  97],\n         [ 63,  79,  95],\n         [ 66,  81,  97],\n         ...,\n         [ 73,  81,  94],\n         [ 70,  78,  91],\n         [ 69,  77,  90]],\n \n        [[ 58,  73,  89],\n         [ 53,  68,  84],\n         [ 55,  68,  84],\n         ...,\n         [ 80,  86,  97],\n         [ 75,  81,  92],\n         [ 73,  79,  90]],\n \n        ...,\n \n        [[ 20,  21,  42],\n         [ 18,  19,  40],\n         [ 20,  21,  42],\n         ...,\n         [ 80,  88, 105],\n         [ 81,  89, 106],\n         [ 83,  91, 108]],\n \n        [[ 55,  52,  77],\n         [ 55,  53,  75],\n         [ 52,  50,  72],\n         ...,\n         [ 81,  88, 105],\n         [ 80,  87, 104],\n         [ 79,  86, 103]],\n \n        [[ 71,  68,  93],\n         [ 72,  69,  94],\n         [ 66,  64,  86],\n         ...,\n         [ 81,  88, 105],\n         [ 77,  84, 101],\n         [ 75,  82,  99]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_25_png.rf.5ea0dd56e58ae897e3fbb1565808c72c.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.3773441314697266, 'inference': 6.484270095825195, 'postprocess': 1.2445449829101562},\n ultralytics.engine.results.Results object with attributes:\n \n boxes: ultralytics.engine.results.Boxes object\n keypoints: None\n masks: None\n names: {0: 'c'}\n obb: None\n orig_img: array([[[ 48,  46,  52],\n         [ 61,  61,  67],\n         [ 74,  76,  86],\n         ...,\n         [ 67,  80, 102],\n         [ 73,  86, 108],\n         [ 77,  90, 112]],\n \n        [[ 41,  39,  45],\n         [ 52,  52,  58],\n         [ 64,  66,  76],\n         ...,\n         [ 70,  83, 105],\n         [ 74,  87, 109],\n         [ 75,  88, 110]],\n \n        [[ 37,  33,  39],\n         [ 45,  43,  49],\n         [ 51,  52,  62],\n         ...,\n         [ 78,  91, 113],\n         [ 78,  91, 113],\n         [ 74,  87, 109]],\n \n        ...,\n \n        [[ 35,  37,  47],\n         [ 39,  44,  53],\n         [ 38,  43,  52],\n         ...,\n         [ 78,  89, 111],\n         [ 72,  83, 105],\n         [ 66,  77,  99]],\n \n        [[ 32,  35,  43],\n         [ 31,  37,  44],\n         [ 33,  38,  47],\n         ...,\n         [ 78,  86, 109],\n         [ 73,  81, 104],\n         [ 69,  77, 100]],\n \n        [[ 34,  38,  43],\n         [ 33,  39,  44],\n         [ 39,  44,  53],\n         ...,\n         [ 69,  77, 100],\n         [ 70,  76,  99],\n         [ 71,  77, 100]]], dtype=uint8)\n orig_shape: (416, 416)\n path: '/kaggle/input/sorghum-crop-line-detection-dataset/data/sorghumfield.v3-416x416_augmented.yolov8/test/images/validation_48_png.rf.a9d3e2d8f0ae863c1589fcd8f21c9c3c.jpg'\n probs: None\n save_dir: 'runs/detect/predict'\n speed: {'preprocess': 1.2638568878173828, 'inference': 6.303548812866211, 'postprocess': 1.2125968933105469}]"},"metadata":{}}],"execution_count":14}]}